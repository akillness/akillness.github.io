---
title: Introducing Phi-3 WebGPU
description: LLM, Phi-3, WebGPU
categories: [LLM, Phi-3]
tags: [LLM, Phi-3, WebGPU]
# author: Xenova
date: 2024-05-08 21:00:00 +0800
# mermaid: true
# render_with_liquid: false
# image:
#   path: /assets/img/llm/LLM_evaluation_rank.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [Rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ]
---

## Introducing Phi-3 WebGPU, a private and powerful AI chatbot that runs locally in your browser, powered by ğŸ¤— Transformers.js and onnxruntime-web!

> ğŸ”’ On-device inference: no data sent to a server
âš¡ï¸ WebGPU-accelerated (> 20 t/s)
ğŸ“¥ Model downloaded once and cached
{: .prompt-info }

Phi-3 running at 42 tokens per second 100% locally in your browser! ğŸ¤¯âš¡ï¸

What speed do you get? 

Try it out! ğŸ‘‡  ( from [Xenova](https://twitter.com/xenovacom))

<https://huggingface.co/spaces/Xenova/experimental-phi3-webgpu>