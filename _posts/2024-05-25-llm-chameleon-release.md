---
title: Release Chameleon Model
description: LLM, Chameleon
categories: [Script, Chameleon]
tags: [Chameleon, LLM]
# author: foDev_jeong
date: 2024-05-25 22:40:00 +0800
mermaid: true
# render_with_liquid: false
image:
  path: /assets/img/llm/LLM_chameleon.jpeg
  lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
  alt: [ Chameleon Architecture ]
---


## Chameleon: Mixed-Modal Early-Fusion Foundation Models

*Curiosity:* Will Chameleon be [Meta](https://www.linkedin.com/company/meta/) Llama 4? ğŸ¦ ğŸ¦™ Meta proposes â€œChameleon: Mixed-Modal Early-Fusion Foundation Modelsâ€ with a unified approach for fully token-based representations of both image and text. No Encoders or connectors. ğŸ‘€



### Architecture Overview

*Retrieve:* Chameleon uses a unified token-based approach for multimodal understanding and generation.

```mermaid
graph TB
    A[Input] --> B[Image Tokenizer]
    A --> C[Text Tokenizer]
    B --> D[1024 Image Tokens]
    C --> E[Text Tokens]
    D --> F[Unified Token Sequence]
    E --> F
    F --> G[Llama 2 Decoder]
    G --> H[Output: Text/Image]
    
    style A fill:#e1f5ff
    style F fill:#fff3cd
    style H fill:#d4edda
```

### Implementation Details

| Step | Component | Details |
|:-----|:----------|:--------|
| **1. Tokenizers** | Image + Text | Image: 512Ã—512 â†’ 1024 tokens (codebook 8192)<br>Text: BPE vocab 65,536 (includes image tokens) |
| **2. Architecture** | Llama 2 Decoder | Query-key normalization<br>Layer norm reordering<br>Stabilized mixed-modal training |
| **3. Pretraining Stage 1** | 80% of training | Text-only: 2.9T tokens<br>Text-image: 1.4B pairs/1.5T tokens<br>Interleaved: 400B tokens |
| **4. Pretraining Stage 2** | 20% of training | Higher quality data<br>Instruction data<br>Half dataset size |
| **5. Fine-tuning** | Final stage | ~1.8M samples<br>~100k vision samples |

### Training Data Breakdown

```mermaid
pie title Training Data Distribution
    "Text-only (2.9T)" : 60
    "Text-Image (1.5T)" : 31
    "Interleaved (400B)" : 9
```

### Key Insights

*Retrieve:* Chameleon's unified token-based approach enables native multimodal understanding and generation.

| Insight | Description | Impact |
|:--------|:------------|:-------|
| **Unified Tokens** | No encoders/connectors | â¬†ï¸ Native multimodal generation |
| **Training Scale** | 9.2T tokens, 2.1 epochs | â¬†ï¸ Strong performance |
| **Code Data** | Improved reasoning | â¬†ï¸ Text-only tasks |
| **Scaling Challenge** | Difficult above 8B/1T | âš ï¸ Training stability |
| **High-Quality Data** | Last 20% crucial | â¬†ï¸ Significant boost |
| **Performance** | Outperforms competitors | â¬†ï¸ Strong results |

### Performance Comparison

*Innovate:* Chameleon-34B achieves competitive performance across benchmarks.

**Text Tasks**:
- Outperforms Llama2-70B
- Approaches Mixtral 8x7B/Gemini-Pro
- Strong on GSM8K, MATH, MMLU

**Vision Tasks**:
- Outperforms Flamingo-80B and IDEFICS-80B on MS-COCO
- Matches performance on Flickr30k

**Multimodal Evaluation**:
- 60.4% win rate vs. Gemini-Pro
- 51.6% win rate vs. GPT-4V

### Comparison with Previous MLLMs

| Model | Architecture | Multimodal Generation |
|:------|:-------------|:----------------------|
| **Idefics, GPT-4v, Flamingo** | Encoders + Connectors | âŒ Limited |
| **Chameleon** | Unified Tokens | âœ… Native support |

**Key Advantage**: Chameleon can generate both text and images using discrete tokens, enabling true multimodal document generation.

### Key Takeaways

*Retrieve:* Chameleon demonstrates that unified token-based representations can achieve strong multimodal performance without separate encoders or connectors.

*Innovate:* By using discrete tokens for both images and text, Chameleon enables native multimodal understanding and generation, approaching GPT-4o's capabilities with a simpler architecture.

*Curiosity â†’ Retrieve â†’ Innovation:* Start with curiosity about unified multimodal models, retrieve insights from Chameleon's token-based approach, and innovate by building applications that leverage native multimodal generation.

**Next Steps**:
- Read the full paper
- Explore Chameleon architecture
- Compare with GPT-4o
- Build multimodal applications

> Paper: <https://huggingface.co/papers/2405.09818>
{: .prompt-info }

Note: Chameleon looks to be closer to [OpenAI](https://www.linkedin.com/company/openai/) GPT-4o than Uni-MoE (shared yesterday) with its native multi-modal tokens. ğŸ’¡


<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

## Chameleon: Mixed-Modal Early-Fusion Foundation Models

ì¹´ë©œë ˆì˜¨ì€ ë¼ë§ˆ 4Meta ë ê¹Œìš”? ğŸ¦ ğŸ¦™ [Meta](https://www.linkedin.com/company/meta/)ëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ëª¨ë‘ë¥¼ ì™„ì „íˆ í† í° ê¸°ë°˜ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•œ í†µí•© ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ "Chameleon: Mixed-Modal Early-Fusion Foundation Models"ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì¸ì½”ë” ë˜ëŠ” ì»¤ë„¥í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ‘€

### Implementation:
- 1ï¸âƒ£ í›ˆë ¨ëœ 2ê°œì˜ í† í¬ë‚˜ì´ì €, 512 Ã— 512 ì´ë¯¸ì§€ë¥¼ ì½”ë“œë¶(8192)ì—ì„œ 1024ê°œì˜ í† í°ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ì´ë¯¸ì§€ í† í¬ë‚˜ì´ì €ì™€ 8192 ì´ë¯¸ì§€ ì½”ë“œë¶ í† í°ì„ í¬í•¨í•˜ëŠ” 65,536ì˜ ì–´íœ˜ë¥¼ ê°€ì§„ BPE.
- 2ï¸âƒ£ëŠ” Llama 2ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë””ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ì¿¼ë¦¬ í‚¤ ì •ê·œí™” ë° ë ˆì´ì–´ ê·œë²”ì˜ ì¬ì •ë ¬ì„ í†µí•©í•˜ì—¬ í˜¼í•© ëª¨ë‹¬ ì„¤ì •ì—ì„œ í›ˆë ¨ì„ ì•ˆì •í™”í•©ë‹ˆë‹¤.
- 3ï¸âƒ£ í…ìŠ¤íŠ¸ ì „ìš©(Llama 2, CodeLlama â‡’ 2.9T í† í°), í…ìŠ¤íŠ¸ ì´ë¯¸ì§€(1.4B ìŒ/1.5T í† í°), í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì¸í„°ë¦¬ë¸Œ(400B í† í°)ì— ëŒ€í•œ ì‚¬ì „ í•™ìŠµ 1ë‹¨ê³„(80%);
- 4ï¸âƒ£ ì‚¬ì „ í•™ìŠµ 2ë‹¨ê³„ (20%) ì²« ë²ˆì§¸ ë‹¨ê³„ì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê³  ë” ë†’ì€ í’ˆì§ˆì˜ ë°ì´í„°ì™€ ì§€ì¹¨ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
- 5ï¸âƒ£ ~100k ë¹„ì „ ìƒ˜í”Œë¡œ ~180ë§Œ ê°œì˜ ìƒ˜í”Œì— ë¯¸ì„¸ ì¡°ì •.

### Insights:
- ğŸ”— ì´ì „ MLLM(Idefics, GPT-4v, Flamingo)ì€ ë©€í‹°ëª¨ë‹¬ë¦¬í‹°ë¥¼ ìœ„í•´ ì¸ì½”ë”ì™€ ì»¤ë„¥í„°ë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ(ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì¶œë ¥)ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤.
- ğŸ¦ ì¹´ë©œë ˆì˜¨ì€ ê°œë³„ í† í°ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì´í•´í•˜ê³  ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ğŸ“š Chameleon-34BëŠ” ì´ 9.2T í† í°ì— ëŒ€í•´ ì „ì²´ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì—ì„œ 2.1 epoch ë™ì•ˆ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤.
- ğŸ”§ ì½”ë“œ ë°ì´í„°ëŠ” í…ìŠ¤íŠ¸ ì „ìš© ì¶”ë¡  ì‘ì—… ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
- âš–ï¸ ì¹´ë©œë ˆì˜¨ ëª¨ë¸ì„ 8B ë§¤ê°œë³€ìˆ˜ ë° 1T í† í° ì´ìƒìœ¼ë¡œ í™•ì¥í•  ë•Œ ì•ˆì •ì ì¸ í›ˆë ¨ì„ ìœ ì§€í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤.
- ğŸš€ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì‚¬ì „ í•™ìŠµì˜ ë§ˆì§€ë§‰ 20%ëŠ” ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- ğŸ† Chameleon-34BëŠ” Llama2-70Bë¥¼ ëŠ¥ê°€í•˜ë©° Mixtral 8x7B/Gemini-Pro, GSM8K, MATH ë° MMLUì— ê·¼ì ‘í•©ë‹ˆë‹¤.
- ğŸ“Š Chameleon-34BëŠ” MS-COCOì—ì„œ Flamingo-80B ë° IDEFICS-80Bë¥¼ ëŠ¥ê°€í•˜ë©° Flickr30kì—ì„œë„ ì¼ì¹˜í•©ë‹ˆë‹¤.
- ğŸ¯ Chameleon-34BëŠ” Gemini-Proë¥¼ ìƒëŒ€ë¡œ 60.4%, GPT-4Vë¥¼ ìƒëŒ€ë¡œ 51.6%ì˜ ìŠ¹ë¥ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- âš–ï¸ ê· í˜• ì¡íŒ ëª¨ë‹¬ë¦¬í‹° ë°ì´í„° ì„¸íŠ¸ëŠ” ë¯¸ì„¸ ì¡°ì • ë° ì •ë ¬ì— ì¤‘ìš”í•©ë‹ˆë‹¤.

> Paper: <https://huggingface.co/papers/2405.09818>
{: .prompt-info }

ì°¸ê³ : ì¹´ë©œë ˆì˜¨ì€ ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ í† í°ì´ ìˆëŠ” Uni-MoE(ì–´ì œ ê³µìœ )ë³´ë‹¤ [OpenAI](https://www.linkedin.com/company/openai/) GPT-4oì— ë” ê°€ê¹Œì›Œ ë³´ì…ë‹ˆë‹¤. ğŸ’¡

</details>