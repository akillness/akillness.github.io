---
title: How can we further mitigate inference bottlenecks in large LLMs
description: Mitigate inference bottleneck
categories: [LLM, Research]
tags: [LLM, Research]
# author: foDev_jeong
date: 2024-05-26 12:10:00 +0800
# mermaid: true
# render_with_liquid: false
# image:
#   path: /assets/img/llm/LLM_evaluation_rank.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [Rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ]
---

How can we further mitigate inference bottlenecks in large #LLMs like Llama3 to enable real-time applications with minimal latency and performance degradation?

### Let's Explore the solution:

#### 1. Model Selection and Optimization: ğŸ§©

- â˜… Model Selection: Carefully consider the trade-off between model complexity and desired performance. Explore smaller, more efficient versions of Llama3 or similar models if real-time response is paramount. âš¡
- â˜… Pruning: Remove redundant or unimportant weights and connections within the LLM to reduce its overall size and computational footprint. This can be achieved through techniques like magnitude pruning or channel pruning. ğŸ”¥
- â˜… Quantization: Reduce the precision of the model's weights and activations from 32-bit floats to lower precision formats (e.g., 16-bit floats or even 8-bit integers) without significant accuracy loss. This can significantly improve inference speed on compatible hardware. ğŸğŸ’¨
- â˜… Knowledge Distillation: Train a smaller, faster student model to mimic the behavior of the larger, more complex teacher model (Llama3). The student model can then be used for real-time applications while retaining the benefits of the larger model. ğŸ§ â©

#### 2. Hardware Acceleration: ğŸš€

- â˜… GPUs: Leverage powerful Graphics Processing Units (GPUs) specifically designed for parallel processing tasks like LLM inference. Utilize frameworks like TensorFlow or PyTorch that offer optimized GPU kernels for efficient LLM execution. ğŸ’»ğŸ“ˆ
- â˜… TPUs: Consider Tensor Processing Units (TPUs) designed by companies like Google specifically for machine learning workloads. TPUs offer even higher performance gains for specific inference tasks compared to traditional GPUs. ğŸ”¥ğŸ’»
- â˜… Specialized Hardware: Explore emerging hardware solutions like AI accelerators or custom chips specifically designed for efficient LLM inference. These can offer significant performance improvements over traditional CPUs and GPUs. ğŸ¤–âš¡

#### 3. Caching and Preprocessing: â³

- â˜… Caching: Implement caching mechanisms to store frequently used model outputs or intermediate results. This can significantly reduce inference time for repetitive queries. ğŸ’¾ğŸ”„
- â˜… Preprocessing: Preprocess user inputs before feeding them to the LLM. This can involve tasks like tokenization, normalization, and dimensionality reduction, all of which can be done offline and improve overall inference speed. âš’ğŸ‘·

#### 4. Model Parallelism and Batching: ğŸ”¥ğŸ”¥

- â˜… Model Parallelism: Distribute the LLM across multiple processing units (GPUs or TPUs) to parallelize inference tasks. This can significantly reduce inference time for large models. ğŸ’»ğŸ’»ğŸ’»
- â˜… Batching: Process multiple user inputs simultaneously in batches instead of individually. This leverages the inherent parallelism of hardware and improves overall throughput. âš¡ğŸ”¥

#### 5. Efficient Software Design: ğŸ’»ğŸ‘¨â€ğŸ’»

- â˜… Code Optimization: Profile and optimize the code used for LLM inference to identify and eliminate performance bottlenecks within the software itself. ğŸ”ğŸ”§
- â˜… Model Serving Frameworks: Utilize efficient model serving frameworks like TensorFlow Serving, PyTorch Serving, or Triton Inference Server to manage LLM deployment and optimize resource utilization. ğŸ›°ï¸ğŸš€

P.S. More in the comment.


![ LLM Research Trends ](/assets/img/llm/LLM_mitigate_inference_bottleneck.jpeg){: .light .w-75 .shadow .rounded-10 w='1212' h='668' }

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 


Llama3ì™€ ê°™ì€ ëŒ€ê·œëª¨ LLMs ì—ì„œ ì¶”ë¡  ë³‘ëª© í˜„ìƒì„ ë”ìš± ì™„í™”í•˜ì—¬ ì§€ì—° ì‹œê°„ê³¼ ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬í˜„í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?

### ì†”ë£¨ì…˜ì„ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.

#### 1. ëª¨ë¸ ì„ íƒ ë° ìµœì í™”: ğŸ§©

- â˜… ëª¨ë¸ ì„ íƒ: ëª¨ë¸ ë³µì¡ì„±ê³¼ ì›í•˜ëŠ” ì„±ëŠ¥ ê°„ì˜ ê· í˜•ì„ ì‹ ì¤‘í•˜ê²Œ ê³ ë ¤í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì‘ë‹µì´ ê°€ì¥ ì¤‘ìš”í•œ ê²½ìš° ë” ì‘ê³  íš¨ìœ¨ì ì¸ Llama3 ë˜ëŠ” ìœ ì‚¬í•œ ëª¨ë¸ì„ ì‚´í´ë³´ì‹­ì‹œì˜¤. âš¡
- â˜… ì •ë¦¬(Pruning): LLM ë‚´ì—ì„œ ì¤‘ë³µë˜ê±°ë‚˜ ì¤‘ìš”í•˜ì§€ ì•Šì€ ê°€ì¤‘ì¹˜ì™€ ì—°ê²°ì„ ì œê±°í•˜ì—¬ ì „ì²´ í¬ê¸°ì™€ ì—°ì‚° ê³µê°„ì„ ì¤„ì…ë‹ˆë‹¤. ì´ëŠ” í¬ê¸° ê°€ì§€ì¹˜ê¸° ë˜ëŠ” ì±„ë„ ê°€ì§€ì¹˜ê¸°ì™€ ê°™ì€ ê¸°ìˆ ì„ í†µí•´ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ”¥
- â˜… ì–‘ìí™”: í° ì •í™•ë„ ì†ì‹¤ ì—†ì´ 32ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì ì—ì„œ ë” ë‚®ì€ ì •ë°€ë„ í˜•ì‹(ì˜ˆ: 16ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì  ë˜ëŠ” 8ë¹„íŠ¸ ì •ìˆ˜)ìœ¼ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° í™œì„±í™”ì˜ ì •ë°€ë„ë¥¼ ì¤„ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ í˜¸í™˜ë˜ëŠ” í•˜ë“œì›¨ì–´ì—ì„œ ì¶”ë¡  ì†ë„ê°€ í¬ê²Œ í–¥ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸğŸ’¨
- â˜… ì§€ì‹ ì¦ë¥˜: ë” ì‘ê³  ë” ë¹ ë¥¸ í•™ìƒ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë” í¬ê³  ë³µì¡í•œ êµì‚¬ ëª¨ë¸(Llama3)ì˜ ë™ì‘ì„ ëª¨ë°©í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ í•™ìƒ ëª¨ë¸ì„ ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì‚¬ìš©í•˜ë©´ì„œ ë” í° ëª¨ë¸ì˜ ì´ì ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ§ â©

#### 2. í•˜ë“œì›¨ì–´ ê°€ì†: ğŸš€

- â˜… GPU: LLM ì¶”ë¡ ê³¼ ê°™ì€ ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—…ì„ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ ê°•ë ¥í•œ ê·¸ë˜í”½ ì²˜ë¦¬ ì¥ì¹˜(GPU)ë¥¼ í™œìš©í•©ë‹ˆë‹¤. íš¨ìœ¨ì ì¸ LLM ì‹¤í–‰ì„ ìœ„í•´ ìµœì í™”ëœ GPU ì»¤ë„ì„ ì œê³µí•˜ëŠ” TensorFlow ë˜ëŠ” PyTorchì™€ ê°™ì€ í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤. ğŸ’»ğŸ“ˆ
- â˜… TPU: Googleê³¼ ê°™ì€ íšŒì‚¬ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬ë¡œë“œë¥¼ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„í•œ TPU(Tensor Processing Unit)ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. TPUëŠ” ê¸°ì¡´ GPUì— ë¹„í•´ íŠ¹ì • ì¶”ë¡  ì‘ì—…ì— ëŒ€í•´ í›¨ì”¬ ë” ë†’ì€ ì„±ëŠ¥ í–¥ìƒì„ ì œê³µí•©ë‹ˆë‹¤. ğŸ”¥ğŸ’»
- â˜… íŠ¹ìˆ˜ í•˜ë“œì›¨ì–´: íš¨ìœ¨ì ì¸ LLM ì¶”ë¡ ì„ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ AI ê°€ì†ê¸° ë˜ëŠ” ë§ì¶¤í˜• ì¹©ê³¼ ê°™ì€ ìƒˆë¡œìš´ í•˜ë“œì›¨ì–´ ì†”ë£¨ì…˜ì„ ì‚´í´ë³´ì„¸ìš”. ì´ëŠ” ê¸°ì¡´ CPU ë° GPUì— ë¹„í•´ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ¤–âš¡

#### 3. ìºì‹± ë° ì „ì²˜ë¦¬: â³

- â˜… ìºì‹±: ìì£¼ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ ì¶œë ¥ ë˜ëŠ” ì¤‘ê°„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ìºì‹± ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°˜ë³µì ì¸ ì¿¼ë¦¬ì— ëŒ€í•œ ì¶”ë¡  ì‹œê°„ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ’¾ğŸ”„
- â˜… ì „ì²˜ë¦¬(Preprocessing): ì‚¬ìš©ì ì…ë ¥ì„ LLMì— ê³µê¸‰í•˜ê¸° ì „ì— ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” í† í°í™”, ì •ê·œí™” ë° ì°¨ì› ì¶•ì†Œì™€ ê°™ì€ ì‘ì—…ì´ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë©°, ì´ ëª¨ë“  ì‘ì—…ì„ ì˜¤í”„ë¼ì¸ì—ì„œ ìˆ˜í–‰í•˜ê³  ì „ë°˜ì ì¸ ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. âš’ğŸ‘·

#### 4. ëª¨ë¸ ë³‘ë ¬ ì²˜ë¦¬ ë° ì¼ê´„ ì²˜ë¦¬: ğŸ”¥ğŸ”¥

- â˜… ëª¨ë¸ ë³‘ë ¬ ì²˜ë¦¬: LLMì„ ì—¬ëŸ¬ ì²˜ë¦¬ ì¥ì¹˜(GPU ë˜ëŠ” TPU)ì— ë¶„ì‚°í•˜ì—¬ ì¶”ë¡  ì‘ì—…ì„ ë³‘ë ¬í™”í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ì¶”ë¡  ì‹œê°„ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ’»ğŸ’»ğŸ’»
- â˜… ì¼ê´„ ì²˜ë¦¬: ì—¬ëŸ¬ ì‚¬ìš©ì ì…ë ¥ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ì¼ê´„ ì²˜ë¦¬í•˜ì—¬ ë™ì‹œì— ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ í•˜ë“œì›¨ì–´ì˜ ê³ ìœ í•œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í™œìš©í•˜ê³  ì „ì²´ ì²˜ë¦¬ëŸ‰ì´ í–¥ìƒë©ë‹ˆë‹¤. âš¡ğŸ”¥

#### 5. íš¨ìœ¨ì ì¸ ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„: ğŸ’»ğŸ‘¨â€ğŸ’» â€

- â˜… ì½”ë“œ ìµœì í™”: LLM ì¶”ë¡ ì— ì‚¬ìš©ë˜ëŠ” ì½”ë“œë¥¼ í”„ë¡œíŒŒì¼ë§í•˜ê³  ìµœì í™”í•˜ì—¬ ì†Œí”„íŠ¸ì›¨ì–´ ìì²´ ë‚´ì˜ ì„±ëŠ¥ ì €í•˜ ìš”ì¸(bottleneck)ì„ ì‹ë³„í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. í”„ë¡œíŒŒì¼ë§ì€ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ê° ë¶€ë¶„ì´ ì†Œìš”ë˜ëŠ” ì‹œê°„ì„ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ğŸ”ğŸ”§
- â˜…  ëª¨ë¸ ì„œë¹™ í”„ë ˆì„ì›Œí¬: TensorFlow Serving, PyTorch Serving, Triton Inference Serverì™€ ê°™ì€ íš¨ìœ¨ì ì¸ ëª¨ë¸ ì„œë¹™ í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ LLM ë°°í¬ë¥¼ ê´€ë¦¬í•˜ê³  ë¦¬ì†ŒìŠ¤ í™œìš©ë„ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í”„ë ˆì„ì›Œí¬ëŠ” LLM ëª¨ë¸ì„ ë¡œë”©, ì‹¤í–‰, ê´€ë¦¬í•˜ëŠ” ê³¼ì •ì„ ê°„ì†Œí™”í•˜ì—¬ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ğŸ›°ï¸ğŸš€

ì¶”ì‹  : ëŒ“ê¸€ì— ë” ë§ì€ ê²ƒì´ ìˆìŠµë‹ˆë‹¤.

</details>