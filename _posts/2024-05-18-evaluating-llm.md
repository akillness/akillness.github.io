---
title: How to evaluate LLM Model?
description: LLM, Evaluation, Framework
categories:
- Development & Tools
tags:
- llm
- evaluation
- development-tools
- tools
date: 2024-05-18 10:20:00 +0800
mermaid: true
---
## LLM evaluation frameworks & tools every AI/ML engineer should know.

*Curiosity:* What insights can we retrieve from this? How does this connect to innovation in the field?

![ LLM evaluation frameworks ](/assets/img/news/Evaluate_llm_guid.png){: .light .w-75 .shadow .rounded-10 w='1212' h='668' }

LLM evaluation frameworks and tools are important because they provide standardized benchmarks to measure and improve the performance, reliability and fairness of language models. 

Also, it is very important to have metrics in place to evaluate LLMs. These metrics act as scoring mechanisms that assess an LLMâ€™s outputs based on the given criteria. 

Here is my article on evaluating large language models. ğŸ‘‰<https://levelup.gitconnected.com/evaluating-large-language-models-a-developers-guide-ffd21a055feb>


## MMLU-Pro released by TIGER-Lab on Hugging Face, continues these vital efforts by offering a more robust and challenging massive multi-task language understanding dataset! ğŸ‰ ğŸ˜

*Curiosity:* Evaluating LLMs is both crucial and challenging, especially with existing benchmarks like MMLU reaching saturation. 



> TL;DR: ğŸ“Š
- ğŸ“š 12K complex questions across various disciplines with careful human verification
- ğŸ”¢ Augmented to 10 options per question (instead of 4) to reduce random guessing
- ğŸ“Š 56% of questions from MMLU, 34% from STEM websites, and the rest from TheoremQA, and SciBench
- ğŸ” Performance drops without chain-of-thought reasoning, indicating a more challenging benchmark!
{: .prompt-tip }


> Results compared to MMLU
- ğŸ“‰ GPT-4o drops by 17% (from 0.887 to 0.7149)
- ğŸ“‰ Mixtral 8x7B drops by 31% (from 0.714 to 0.404)
- ğŸ“‰ Llama-3-70B drops by 27% (from 0.820 to 0.5541)
{: .prompt-tip }



1. Dataset: <https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro>
2. Leaderboard: <https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro#4-leaderboard>


<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 


## ëª¨ë“  AI/ML ì—”ì§€ë‹ˆì–´ê°€ ì•Œì•„ì•¼ í•  í•´ì‹œíƒœê·¸#LLM í‰ê°€ í”„ë ˆì„ì›Œí¬ ë° ë„êµ¬.

![ LLM evaluation frameworks ](/assets/img/news/Evaluate_llm_guid.png){: .light .w-75 .shadow .rounded-10 w='1212' h='668' }

LLM í‰ê°€ í”„ë ˆì„ì›Œí¬ì™€ íˆ´ì€ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥, ì‹ ë¢°ì„± ë° ê³µì •ì„±ì„ ì¸¡ì •í•˜ê³  ê°œì„ í•˜ê¸° ìœ„í•œ í‘œì¤€í™”ëœ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì¤‘ìš”í•©ë‹ˆë‹¤. 

ë˜í•œ LLMì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë©”íŠ¸ë¦­ì„ ë§ˆë ¨í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë©”íŠ¸ë¦­ì€ ì£¼ì–´ì§„ ê¸°ì¤€ì— ë”°ë¼ LLMì˜ ì¶œë ¥ì„ í‰ê°€í•˜ëŠ” ìŠ¤ì½”ì–´ë§ ë©”ì»¤ë‹ˆì¦˜ ì—­í• ì„ í•©ë‹ˆë‹¤. 

ë‹¤ìŒì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ í‰ê°€ì— ëŒ€í•œ ê¸°ì‚¬ì…ë‹ˆë‹¤. ğŸ‘‰<https://levelup.gitconnected.com/evaluating-large-language-models-a-developers-guide-ffd21a055feb>


## Hugging Face TIGER-Labì—ì„œ ì¶œì‹œí•œ MMLU-ProëŠ” ë³´ë‹¤ ê°•ë ¥í•˜ê³  ë„ì „ì ì¸ ëŒ€ê·œëª¨ ë‹¤ì¤‘ ì‘ì—… ì–¸ì–´ ì´í•´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì œê³µí•˜ì—¬ ì´ëŸ¬í•œ ì¤‘ìš”í•œ ë…¸ë ¥ì„ ê³„ì†í•©ë‹ˆë‹¤! ğŸ‰ ğŸ˜

LLMì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•˜ë©´ì„œë„ ì–´ë ¤ìš´ ì¼ì´ë©°, íŠ¹íˆ MMLUì™€ ê°™ì€ ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ê°€ í¬í™” ìƒíƒœì— ë„ë‹¬í•œ ìƒí™©ì—ì„œëŠ” ë”ìš± ê·¸ë ‡ìŠµë‹ˆë‹¤.

> TL;DR: ğŸ“Š
- ğŸ“š ì‹ ì¤‘í•œ ì¸ì  ê²€ì¦ì„ í†µí•´ ë‹¤ì–‘í•œ ë¶„ì•¼ì— ê±¸ì¹œ 12Kê°œì˜ ë³µì¡í•œ ì§ˆë¬¸
- ğŸ”¢ ë¬´ì‘ìœ„ ì¶”ì¸¡ì„ ì¤„ì´ê¸° ìœ„í•´ ì§ˆë¬¸ë‹¹ 4ê°œê°€ ì•„ë‹Œ 10ê°œì˜ ì˜µì…˜ìœ¼ë¡œ ëŠ˜ì–´ë‚¬ìŠµë‹ˆë‹¤.
- ğŸ“Š ì§ˆë¬¸ì˜ 56%ëŠ” MMLU, 34%ëŠ” STEM ì›¹ì‚¬ì´íŠ¸, ë‚˜ë¨¸ì§€ëŠ” TheoremQA ë° SciBenchì—ì„œ ë‚˜ì™”ìŠµë‹ˆë‹¤.
- ğŸ” ìƒê°ì˜ ì—°ì‡„ ì¶”ë¡  ì—†ì´ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë©°, ì´ëŠ” ë” ì–´ë ¤ìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤!
{: .prompt-tip }

> MMLUì™€ ë¹„êµí•œ ê²°ê³¼
- ğŸ“‰ GPT-4oëŠ” 17% í•˜ë½(0.887ì—ì„œ 0.7149ë¡œ)
- ğŸ“‰ Mixtral 8x7B 31% ê°ì†Œ(0.714ì—ì„œ 0.404ë¡œ)
- ğŸ“‰ ë¼ë§ˆ-3-70B 27% í•˜ë½(0.820ì—ì„œ 0.5541ë¡œ)
{: .prompt-tip }


1. ë°ì´í„° ì„¸íŠ¸: <https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro>
2. ë¦¬ë”ë³´ë“œ: <https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro#4-leaderboard>

</details>