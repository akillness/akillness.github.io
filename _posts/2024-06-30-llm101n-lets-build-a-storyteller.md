---
title: â LLM101n, LLM AI ìŠ¤í† ë¦¬í…”ëŸ¬ êµ¬ì¶•í•˜ê¸° ğŸ› ï¸
description: LLM, AI, Storyteller
categories: [LLM, Course]
tags: [Storyteller, Course]
# author: foDev_jeong
date: 2024-06-30 19:10:00 +0800
# pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---

ë©°ì¹  ì „ Andrej Karpathyì˜ í¥ë¯¸ë¡œìš´ Courseê°€ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.
( ì•„ì§ ë‚´ìš©ì€ ì—…ë°ì´íŠ¸ ì¤‘ì´ì§€ë§Œ, ëª©ì°¨ë‚´ìš©ì„ ì°¸ê³ í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. )

ì´ë²ˆì— ê³µê°œëœ â€œLLM101n: Let's build a Storytellerâ€ ì½”ìŠ¤ì˜ íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

â€¢ ğ—Ÿğ—Ÿğ— ğŸ­ğŸ¬ğŸ­ğ—»: ğ—¢ğ˜ƒğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„
 - ìŠ¤í† ë¦¬í…”ëŸ¬ AI ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì§ì ‘ êµ¬ì¶•
 - ì§ì ‘ êµ¬ì¶• AIë¡œ ì‘ì€ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ê³ , ë‹¤ë“¬ê³ , ì„¤ëª… ê°€ëŠ¥
 - ìµœì†Œí•œì˜ ì»´í“¨í„° ê³¼í•™ ì „ì œ ì¡°ê±´ìœ¼ë¡œ í•™ìŠµ ì‹œì‘
 - ê¸°ì´ˆë¶€í„° ChatGPTì™€ ìœ ì‚¬ ê¸°ëŠ¥ì„ ê°–ì¶˜ ì›¹ ì•±ê¹Œì§€ E2E êµ¬ì¶•
 - ì£¼ìš” ê°œë°œ ì–¸ì–´ë¡œëŠ” Python, C ë° CUDAë¡œ ì§„í–‰
 - Courseë¥¼ í†µí•´ AI, LLM, ë”¥ëŸ¬ë‹ ì „ë°˜ì— ëŒ€í•´ ê¹Šê²Œ ì´í•´ ê°€ëŠ¥

íŠ¹íˆ ì´ ğ—Ÿğ—Ÿğ— ğŸ­ğŸ¬ğŸ­ğ—» ì½”ìŠ¤ëŠ” ëª¨ë“ ê±¸ ë°”ë‹¥ë¶€í„° end-to-endë¡œ ë‹¤ ê°œë°œí•´ ë³¸ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆì–´, ì‹¤ìŠµ ë°”íƒ•ì˜ ì‚¬ë‚´ ì‹¤ë¬´ êµìœ¡ ê³¼ì •ìœ¼ë¡œë„ ì¢‹ì•„ ë³´ì…ë‹ˆë‹¤. ğŸ¤”

â€¢ ğ—Ÿğ—Ÿğ— ğŸ­ğŸ¬ğŸ­ğ—»: ğ—¦ğ˜†ğ—¹ğ—¹ğ—®ğ—¯ğ˜‚ğ˜€
 - Chapter 01 Bigram Language Model (language modeling)
 - Chapter 02 Micrograd (machine learning, backpropagation)
 - Chapter 03 N-gram model (multi-layer perceptron, matmul, gelu)
 - Chapter 04 Attention (attention, softmax, positional encoder)
 - Chapter 05 Transformer (transformer, residual, layernorm, GPT-2)
 - Chapter 06 Tokenization (minBPE, byte pair encoding)
 - Chapter 07 Optimization (initialization, optimization, AdamW)
 - Chapter 08 Need for Speed I: Device (device, CPU, GPU, ...)
 - Chapter 09 Need for Speed II: Precision (mixed precision training, fp16, bf16, fp8, ...)
 - Chapter 10 Need for Speed III: Distributed (distributed optimization, DDP, ZeRO)
 - Chapter 11 Datasets (datasets, data loading, synthetic data generation)
 - Chapter 12 Inference I: kv-cache (kv-cache)
 - Chapter 13 Inference II: Quantization (quantization)
 - Chapter 14 Finetuning I: SFT (supervised finetuning SFT, PEFT, LoRA, chat)
 - Chapter 15 Finetuning II: RL (reinforcement learning, RLHF, PPO, DPO)
 - Chapter 16 Deployment (API, web app)
 - Chapter 17 Multimodal (VQVAE, diffusion transformer)

â€¢ ğ—”ğ—½ğ—½ğ—²ğ—»ğ—±ğ—¶ğ˜…: ì•„ë˜ ì£¼ì œë“¤ì´ ìœ„ Courseì— ì¶”ê°€ë  ì˜ˆì •ì´ë¼ í•©ë‹ˆë‹¤.
 - Programming languages: Assembly, C, Python
 - Data types: Integer, Float, String (ASCII, Unicode, UTF-8)
 - Tensor: shapes, views, strides, contiguous, ...
 - Deep Learning frameowrks: PyTorch, JAX
 - Neural Net Architecture: GPT (1,2,3,4), Llama (RoPE, RMSNorm, GQA), MoE, ...
 - Multimodal: Images, Audio, Video, VQVAE, VQGAN, diffusion

ì°¸ê³ ë¡œ Teslaì—ì„œ ì¸ê³µ ì§€ëŠ¥ ë° Autopilot Vision ë¶€ë¬¸ì„ ë‹´ë‹¹í–ˆë˜ KarpathyëŠ” 2023ë…„ Teslaë¥¼ ë– ë‚œ ì´í›„ OpenAIì— í•©ë¥˜, ë‹¤ì‹œ OpenAIë¥¼ ë– ë‚˜ ê·¼ë˜ì—ëŠ” ëŒ€ì¤‘ì„ ìœ„í•œ ì—¬ëŸ¬ AI ìë£Œë“¤ì„ ë§Œë“¤ì–´ ì œê³µí•´ ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.

ì§„ì •í•œ "ëª¨ë‘ì˜ ì¸ê³µì§€ëŠ¥"ì„ ì‹¤í˜„í•˜ê³  ìˆëŠ” Andrej Karpathy, ì¡´ê²½ìŠ¤ëŸ½ë„¤ìš”.

(Source) Andrej Karpathy, Github ğŸ‘ 
 - LM101n: Let's build a Storyteller : <https://github.com/karpathy/LLM101n>

![ LLM101N storyteller AI ](/assets/img/llm/llm101n_storyteller.jpeg){: .light .shadow .rounded-10 w='1212' h='668' }
