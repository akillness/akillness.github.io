---
title: ğŸ’¡ CRAG (Comprehensive RAG) is a new RAG benchmark dataset
description: CRAG, LLM
categories:
- LLM & Language Models
tags:
- rag
- crag
- llm
- language-model
date: 2024-06-22 19:10:00 +0800
mermaid: true
---
## CRAG: Comprehensive RAG Benchmark Dataset

*Curiosity:* How can we create a realistic benchmark for RAG systems? What makes CRAG more challenging than existing datasets?

**CRAG (Comprehensive RAG)** is a new benchmark dataset that provides robust and challenging test cases for evaluating RAG and QA systems. Even GPT-4 struggles, achieving less than 34% accuracy, highlighting the challenge.

> **Paper**: <https://arxiv.org/pdf/2406.04744>
{: .prompt-info}

### The Problem

*Retrieve:* Existing RAG datasets have limitations.

| Issue | Description | Impact |
|:------|:------------|:-------|
| **Lack of Diversity** | Limited question types | âš ï¸ Incomplete evaluation |
| **Complexity Gap** | Don't represent real-world QA | âš ï¸ Suboptimal assessment |
| **Evaluation Issues** | Poor performance metrics | âš ï¸ Unreliable results |

**Result**: Suboptimal performance evaluation of RAG systems.

### CRAG Dataset Overview

*Innovate:* Comprehensive benchmark for RAG evaluation.

```mermaid
graph TB
    A[CRAG Dataset] --> B[4,409 QA Pairs]
    A --> C[5 Domains]
    A --> D[8 Question Categories]
    A --> E[Mock APIs]
    A --> F[Score System]
    
    E --> E1[Web Search]
    E --> E2[KG Search]
    
    F --> F1[Penalize Hallucinations]
    F --> F2[Reliable Evaluation]
    
    style A fill:#e1f5ff
    style B fill:#fff3cd
    style F fill:#d4edda
```

### Dataset Features

*Retrieve:* CRAG's comprehensive features.

| Feature | Details | Benefit |
|:--------|:--------|:--------|
| **QA Pairs** | 4,409 pairs | â¬†ï¸ Large scale |
| **Domains** | 5 domains | â¬†ï¸ Diversity |
| **Categories** | 8 question types | â¬†ï¸ Coverage |
| **Complexity** | Simple facts to complex queries | â¬†ï¸ Real-world |
| **Mock APIs** | Web and KG search | â¬†ï¸ Realistic |
| **Score System** | Penalizes hallucinations | â¬†ï¸ Reliable |

### Evaluation Tasks

*Innovate:* Comprehensive task coverage.

**Task Types**:
- **Web Retrieval**: Realistic web search scenarios
- **Structured Querying**: Knowledge Graph queries
- **Summarization**: Multi-document summarization

**Coverage**: From simple facts to complex multi-hop queries.

### Performance Results

*Retrieve:* CRAG reveals significant challenges.

| System | Accuracy | Notes |
|:-------|:---------|:------|
| **Advanced LLMs (GPT-4)** | <34% | Highlights challenge |
| **Direct RAG** | 44% | Needs improvement |
| **SOTA Industry RAG** | 63% | Without hallucination |

**Key Findings**:
- Even best LLMs struggle (<34%)
- Direct RAG only reaches 44%
- Industry solutions achieve 63% (best case)

### Score System Innovation

*Innovate:* Better evaluation through hallucination penalties.

**Key Feature**: Penalizes hallucinated answers more than missing answers

**Benefits**:
- âœ… Encourages accuracy over completeness
- âœ… Reduces false information
- âœ… More reliable evaluation
- âœ… Better reflects real-world needs

### Key Takeaways

*Retrieve:* CRAG provides a comprehensive benchmark with 4,409 QA pairs across 5 domains and 8 categories, including realistic retrieval scenarios and a score system that penalizes hallucinations.

*Innovate:* By creating a challenging benchmark that even GPT-4 struggles with (<34% accuracy), CRAG encourages development of more advanced RAG solutions, with industry SOTA reaching 63% accuracy.

*Curiosity â†’ Retrieve â†’ Innovation:* Start with curiosity about RAG evaluation, retrieve insights from CRAG's comprehensive approach, and innovate by building RAG systems that can handle the complexity and diversity of real-world QA tasks.

**Next Steps**:
- Read the full paper
- Test your RAG on CRAG
- Analyze performance gaps
- Improve your systems

![ CRAG abstract ](/assets/img/llm/CRAG_abstract.jpeg){: .light .shadow .rounded-10 w='1212' h='668' }

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

## ğŸ˜‰ ë‹¤ìŒì€ RAG íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ì–´ë ¤ìš´ ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤! GPT-4ì™€ ê°™ì€ LLMì¡°ì°¨ë„ 34% ë¯¸ë§Œì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤.

ê¸°ì¡´ RAG ë°ì´í„° ì„¸íŠ¸ëŠ” ë‹¤ì–‘ì„±ì´ ë¶€ì¡±í•˜ê³  ì‹¤ì œ QA ì‘ì—…ì˜ ë³µì¡ì„±ì„ ë‚˜íƒ€ë‚´ì§€ ëª»í•˜ì—¬ ì„±ëŠ¥ í‰ê°€ê°€ ìµœì í™”ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ğŸ’¡ CRAG(Comprehensive RAG)ëŠ” RAG ë° QA ì‹œìŠ¤í…œì„ í‰ê°€í•˜ê¸° ìœ„í•œ ê°•ë ¥í•˜ê³  ë„ì „ì ì¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ìƒˆë¡œìš´ RAG ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ë¡œ, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” LLM ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ì˜ ë°œì „ì„ ì¥ë ¤í•©ë‹ˆë‹¤.

- â›³ CRAGì—ëŠ” 5ê°œ ë„ë©”ì¸ê³¼ 8ê°œ ì§ˆë¬¸ ë²”ì£¼ì— ê±¸ì³ 4,409ê°œì˜ QA ìŒì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ê°„ë‹¨í•œ ì‚¬ì‹¤ë¶€í„° ë³µì¡í•œ ì¿¼ë¦¬ê¹Œì§€ ë‹¤ë£¹ë‹ˆë‹¤.
- â›³ ì›¹ ë° KG(Knowledge Graph) ê²€ìƒ‰ì„ ìœ„í•œ ëª¨ì˜ APIë¥¼ ì œê³µí•˜ì—¬ í˜„ì‹¤ì ì¸ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- â›³ ë¯¸ê²° ë‹µë³€ë³´ë‹¤ í™˜ê°ì— ê±¸ë¦° ë‹µë³€ì— ë” ë§ì€ í˜ë„í‹°ë¥¼ ì£¼ëŠ” ì ìˆ˜ ì‹œìŠ¤í…œì„ ë„ì…í•˜ì—¬ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
- â›³ ì›¹ ê²€ìƒ‰, êµ¬ì¡°ì  ì¿¼ë¦¬ ë° ìš”ì•½ì„ ìœ„í•œ ì‘ì—…ì„ ì œê³µí•˜ì—¬ RAG ì†”ë£¨ì…˜ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê¸°ì—¬
- ğŸ‘‰ ê°€ì¥ ì§„ë³´ëœ LLMì€ ë‹¤ìŒê³¼ ê°™ì€ ì„±ê³¼ë¥¼ ê±°ë‘¡ë‹ˆë‹¤. <34% accuracy on CRAG, highlighting the challenge.
- ğŸ‘‰ Direct application of RAG improves accuracy to only 44%, indicating the need for more advanced solutions.
- ğŸ‘‰ State-of-the-art industry RAG solutions reach 63% accuracy without hallucination.


</details>