---
title: ğğğ° ğğğœğ¨ğğ¢ğ§ğ  ğ­ğğœğ¡ğ§ğ¢ğªğ®ğ ğ¢ğ§ ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğğ«ğ¬ ğ¬ğ¢ğ ğ§ğ¢ğŸğ¢ğœğšğ§ğ­ğ¥ğ² ğ«ğğğ®ğœğğ¬ ğ¡ğšğ¥ğ¥ğ®ğœğ¢ğ§ğšğ­ğ¢ğ¨ğ§ğ¬ ğŸ‘
description: DoLa, Decoding
categories: [LLM, DoLa]
tags: [DoLa, Decoding]
# author: foDev_jeong
date: 2024-07-16 11:30:00 +0800
pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---


### DoLa decoding, which made a conference paper at ICLR '24, has just been merged in Transformers by JoÃ£o Gante and Yung-Sung Chuang.
This new decoding method is simple yet extremely impressive!

Reminder: Decoder LLMs (the GPT kind of LLM, the most common one) generate their outputs one token at a time: at each step, given a current text, they compute, for each token in their vocabulary, a "logit" that should represent the probability that this token is coming next.

Then the decoder either picks the highest logit token (greedy decoding) or samples one with a probability defined by the logits (sampling). The token gets appended to the current text, and the decoder compute logits again, and the cycle continues.

The authors of DoLa wanted to improve that simple method. 

They knew this established fact that transformer LMs encode low-level info (like base syntax) in early layers and more high-level info like knowledge in the later layers.

ğŸ’¡ This gave them their key idea: During decoding, rather than picking the token with the highest logit, ğ˜„ğ—µğ˜† ğ—»ğ—¼ğ˜ ğ—½ğ—¶ğ—°ğ—¸ ğ˜ğ—µğ—² ğ˜ğ—¼ğ—¸ğ—²ğ—» ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ—ºğ—¼ğ˜€ğ˜ ğ—¶ğ—ºğ—½ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ˜ƒğ—² ğ—¶ğ—»ğ—°ğ—¿ğ—²ğ—®ğ˜€ğ—² ğ—¶ğ—» ğ—¹ğ—¼ğ—´ğ—¶ğ˜ ğ—®ğ—°ğ—¿ğ—¼ğ˜€ğ˜€ ğ—¹ğ—®ğ˜†ğ—²ğ—¿ğ˜€?

Implementation is actually quite simple: at each step, you get the layer for which the logits diverge most from your final layer, and this chosen layer becomes the premature layer. Then you subtract the logits from the premature layer to your final layer, in order to reward tokens for which the logits progressed most. And this lets you pick your next token.

Their test settings:
- â¤ Test 4 sizes of Llama-1 models (7b to 65B)
- â¤ Benchmarks on multiple choice QA (TruthfulQA, FACTOR) and open-ended QA (TruthfulQA-open-ended, GSM8K)

âœ¨ ğ—¥ğ—²ğ˜€ğ˜‚ğ—¹ğ˜ğ˜€ ğ—®ğ—¿ğ—² ğ—²ğ˜…ğ˜ğ—¿ğ—²ğ—ºğ—²ğ—¹ğ˜† ğ—¶ğ—ºğ—½ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ˜ƒğ—²!
- ğŸš€ ğŸ±% - ğŸ®ğŸ¬% ğ—¯ğ—®ğ˜€ğ—² ğ—½ğ—¼ğ—¶ğ—»ğ˜ğ˜€ ğ—¶ğ—»ğ—°ğ—¿ğ—²ğ—®ğ˜€ğ—² ğ—®ğ—°ğ—¿ğ—¼ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—¯ğ—²ğ—»ğ—°ğ—µğ—ºğ—®ğ—¿ğ—¸ğ˜€
- ğŸš€ For instance on TruthfulQA / Open-ended, across all model sizes the increase in truthfulness is 14 base points, which is ğ—®ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğŸ°ğŸ¬% ğ—¶ğ—ºğ—½ğ—¿ğ—¼ğ˜ƒğ—²ğ—ºğ—²ğ—»ğ˜ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—²ğ—± ğ˜ğ—¼ ğ˜€ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—± ğ—±ğ—²ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´!

ğŸ¤” Wouldn't decoding take longer because of this added contrasting step?
- ğŸ‘‰ ğ—§ğ—µğ—² ğ—¿ğ˜‚ğ—»ğ˜ğ—¶ğ—ºğ—² ğ—¶ğ—»ğ—°ğ—¿ğ—²ğ—®ğ˜€ğ—² ğ—¶ğ˜€ ğ—»ğ—²ğ—´ğ—¹ğ—¶ğ—´ğ—¶ğ—¯ğ—¹ğ—², ğŸ­ ğ˜ğ—¼ ğŸ´% ğ—¼ğ—»ğ—¹ğ˜†.

The paper has additional insights such as how token confidence evolves across layers for different types of tokens: I recommend to read it!

> ğ™ğ™šğ™–ğ™™ ğ™ğ™© ğ™ğ™šğ™§ğ™š ğŸ‘‰ <https://huggingface.co/papers/2309.03883>
{: .prompt-info}

![ DoLa new Decoding Tech ](/assets/img/llm/DoLa-new-decoding-tech.jpeg){: .light .shadow .rounded-10 w='1212' h='668' }
