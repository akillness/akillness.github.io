---
title: ğŸŒŸ ğ‹ğ‹ğŒ ğğ«ğ¨ğ¯ğ¢ğğğ«'ğ¬ ğ‘ğğ¥ğğšğ¬ğ (2024 1H) & ğŸ“ 2024 ğ‹ğ‹ğŒ ğ’ğ®ğ«ğ¯ğğ² (on Training / Data / RAG / Serving / Agent)
description: LLM, Survey
categories: [LLM, Survey]
tags: [LLM, Survey]
# author: foDev_jeong
date: 2024-05-29 24:00:00 +0800
pin: true
# mermaid: true
# render_with_liquid: false
image:
  path: /assets/img/llm/LLM_Provider_2024.jpeg
  lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
  alt: [LLM 2024 Procider]
---

## ğŸŒŸ ğ‹ğ‹ğŒ ğğ«ğ¨ğ¯ğ¢ğğğ«'ğ¬ ğ‘ğğ¥ğğšğ¬ğ (2024 1H)

I have selected 21 LLMs from various providers for 2024 1H.
[ğŸ“£Release news] and [ğŸ“‹Tech report] are in this post.
[ğŸ“˜API docs] and [ğŸ¤—HF models] are in the comments.

1. ğğ°ğğ§-2 (â€‹Alibaba Groupâ€‹, 2024.06.07)
- â€¢ ğŸ“£News: <https://qwenlm.github.io/blog/qwen2/>

2. ğ’ğ¨ğ¥ğšğ«-ğŒğ¢ğ§ğ¢-ğ£ğš (â€‹Upstageâ€‹, 2024.05.22)
- â€¢ ğŸ“£News: <https://www.upstage.ai/feed/tech/solar-mini-chat-ja>

3. ğ˜ğ¢-ğ‹ğšğ«ğ ğ (â€‹01.AIâ€‹, 2024.05.13)
- â€¢ ğŸ“£News: <https://x.com/01AI_Yi/status/1789929378467426794>

4. ğ˜ğ¢-1.5 (â€‹01.AIâ€‹, 2024.05.13)
- â€¢ ğŸ“£News: <https://x.com/01AI_Yi/status/1789869537317540016>
- â€¢ ğŸ“‹arXiv: <https://arxiv.org/abs/2403.04652>

5. ğ†ğğ“-4ğ¨ (â€‹OpenAIâ€‹, 2024.05.13)
- â€¢ ğŸ“£News: <https://openai.com/index/hello-gpt-4o/>

6. ğğ°ğğ§-ğŒğšğ± (â€‹Alibaba Groupâ€‹, 2024.05.11)
- â€¢ ğŸ“£News: <https://qwenlm.github.io/blog/qwen-max-0428/>

7. ğƒğğğ©ğ’ğğğ¤-ğ•2 (DeepSeek, 2024.05.07)
- â€¢ ğŸ“£News: <https://x.com/deepseek_ai/status/1787478986731429933>
- â€¢ ğŸ“‹arXiv: <https://arxiv.org/abs/2405.04434>

8. ğ’ğ§ğ¨ğ°ğŸğ¥ğšğ¤ğ-ğ€ğ«ğœğ­ğ¢ğœ (â€‹Snowflakeâ€‹, 2024.04.24)
- â€¢ ğŸ“£News: <https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/>

9. ğğ¡ğ¢-3 (â€‹Microsoftâ€‹, 2024.04.22)
- â€¢ ğŸ“£News: <https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/>
- â€¢ ğŸ“‹arXiv: <https://arxiv.org/abs/2404.14219>

10. ğ‹ğ¥ğšğ¦ğš-3 (â€‹Meta Facebookâ€‹, 2024.04.18)
- â€¢ ğŸ“£News: <https://ai.meta.com/blog/meta-llama-3/>

11. ğŒğ¢ğ±ğ­ğ«ğšğ¥-8ğ±22ğ (â€‹Mistral AIâ€‹, 2024.04.17)
- â€¢ ğŸ“£News: <https://mistral.ai/news/mixtral-8x22b/>

12. ğ‘ğğ¤ğš-ğ‚ğ¨ğ«ğ (â€‹Reka AIâ€‹â€‹, 2024.04.15)
- â€¢ ğŸ“£News: <https://www.reka.ai/news/reka-core-our-frontier-class-multimodal-language-model>
- â€¢ ğŸ“‹arXiv: <https://arxiv.org/abs/2404.12387>

13. ğ‚ğ¨ğ¦ğ¦ğšğ§ğ-ğ‘-ğğ¥ğ®ğ¬ (â€‹Cohereâ€‹, 2024.04.04)
- â€¢ ğŸ“£News: <https://cohere.com/blog/command-r-plus-microsoft-azure>

14. ğƒğğ‘ğ— (â€‹Databricksâ€‹, 2024.03.27)
- â€¢ ğŸ“£News: <https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm>

15. ğ†ğğ¦ğ¢ğ§ğ¢-1.5 (â€‹Googleâ€‹, 2024.03.08)
- â€¢ ğŸ“£News: <https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/>
- â€¢ ğŸ“‹arXiv: <https://arxiv.org/abs/2403.05530>

16. ğ‚ğ¥ğšğ®ğğ-3 (â€‹Anthropicâ€‹, 2024.03.04)
- â€¢ ğŸ“£News: <https://www.anthropic.com/news/claude-3-family>

17. ğŒğ¢ğ¬ğ­ğ«ğšğ¥-ğ‹ğšğ«ğ ğ (â€‹Mistral AIâ€‹, 2024.02.26)
- â€¢ ğŸ“£News: <https://mistral.ai/news/mistral-large/>

18. ğ†ğğ¦ğ¦ğš (â€‹Googleâ€‹, 2024.02.21)
- â€¢ ğŸ“£News: <https://blog.google/technology/developers/gemma-open-models/>
- â€¢ ğŸ“‹arXiv: <https://arxiv.org/abs/2403.08295>

19. ğğ°ğğ§-1.5 (â€‹Alibaba Groupâ€‹, 2024.02.04)
- â€¢ ğŸ“£News: <https://qwenlm.github.io/blog/qwen1.5/>

20. ğ’ğ¨ğ¥ğšğ«-ğŒğ¢ğ§ğ¢ (â€‹Upstageâ€‹, 2024.01.25)
- â€¢ ğŸ“£News: <https://www.upstage.ai/feed/product/solarmini-performance-report>

21. ğ’ğ¨ğ¥ğšğ«-10.7ğ (â€‹Upstageâ€‹, 2023.12.23)
- â€¢ ğŸ“£News: <https://www.upstage.ai/feed/press/solar-10-7b-emerges-as-worlds-top-pre-trained-llm>
- â€¢ ğŸ“‹arXiv: <https://arxiv.org/abs/2312.15166>


* * *

## ğŸ“ 2024 ğ‹ğ‹ğŒ ğ’ğ®ğ«ğ¯ğğ² (on Training / Data / RAG / Serving / Agent)

> **I'll read all below !!**
{: .prompt-warning }

![ LLM 2024 survey ](/assets/img/llm/llm-2024-survey.jpeg){: .light .shadow .rounded-10 w='1212' h='668' }

### ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  
ğŸ“Œ A Survey on Self-Evolution of Large Language Models (2024.04.22)
- â€¢ arXiv: <https://arxiv.org/abs/2404.14387>
- â€¢ Github: <https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM>

ğŸ“Œ Continual Learning of Large Language Models: A Comprehensive Survey (2024.04.25)
- â€¢ arXiv: <https://arxiv.org/abs/2404.16789>
- â€¢ Github: <https://github.com/Wang-ML-Lab/llm-continual-learning-survey>

ğŸ“Œ Continual Learning with Pre-Trained Models: A Survey (2024.01.29)
- â€¢ arXiv: <https://arxiv.org/abs/2401.16386>
- â€¢ Github: <https://github.com/sun-hailong/LAMDA-PILOT>

### Dğšğ­ğš
ğŸ“Œ Datasets for Large Language Models: A Comprehensive Survey (2024.02.28)
- â€¢ arXiv: <https://arxiv.org/pdf/2402.18041>
- â€¢ Github: <https://github.com/lmmlzn/Awesome-LLMs-Datasets>

ğŸ“Œ A Survey on Data Selection for Language Models (2024.02.26)
- â€¢ arXiv: <https://arxiv.org/abs/2402.16827>
- â€¢ Github: <https://github.com/alon-albalak/data-selection-survey>

ğŸ“Œ A Survey on Data Selection for LLM Instruction Tuning (2024.02.04)
- â€¢ arXiv: <https://arxiv.org/abs/2402.05123>
- â€¢ Github: <https://github.com/Bolin97/awesome-instruction-selector>

### ğ‘ğ€ğ†
ğŸ“Œ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing (2024.04.30)
- â€¢ arXiv: <https://arxiv.org/abs/2404.19543>
- â€¢ Github: <https://github.com/2471023025/RALM_Survey>

ğŸ“Œ Retrieval-Augmented Generation for AI-Generated Content: A Survey (2024.02.29)
- â€¢ arXiv: <https://arxiv.org/abs/2402.19473>
- â€¢ Github: <https://github.com/hymie122/RAG-Survey>

ğŸ“Œ Retrieval-Augmented Generation for Large Language Models: A Survey (2023.12.18)
- â€¢ arXiv: <https://arxiv.org/abs/2312.10997>
- â€¢ Github: <https://github.com/Tongji-KGLLM/RAG-Survey>

### ğ’ğğ«ğ¯ğ¢ğ§ğ 
ğŸ“Œ LLM Inference Unveiled: Survey and Roofline Model Insights (2024.02.26)
- â€¢ arXiv: <https://arxiv.org/abs/2402.16363>
- â€¢ Github: <https://github.com/hahnyuan/LLM-Viewer>

ğŸ“Œ A Survey on Effective Invocation Methods of Massive LLM Services (2024.02.05)
- â€¢ arXiv: <https://arxiv.org/abs/2402.03408>
- â€¢ Github: <https://github.com/W-caner/Effective-strategy-for-LMaas>

ğŸ“Œ Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models (2024.01.01)
- â€¢ arXiv: <https://arxiv.org/abs/2401.00625>
- â€¢ Github: <https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers>

### ğ€ğ ğğ§ğ­
ğŸ“Œ Large Multimodal Agents: A Survey (2024.02.23)
- â€¢ arXiv: <https://arxiv.org/abs/2402.15116>
- â€¢ Github: <https://github.com/jun0wanan/awesome-large-multimodal-agents>

ğŸ“Œ Large Language Model based Multi-Agents: A Survey of Progress and Challenges (2024.01.21)
- â€¢ arXiv: <https://arxiv.org/abs/2402.01680>
- â€¢ Github: <https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers>

ğŸ“Œ Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security (2024.01.10)
- â€¢ arXiv: <https://arxiv.org/abs/2401.05459>
- â€¢ Github: <https://github.com/MobileLLM/Personal_LLM_Agents_Survey>

