---
title: RAG or Fine Tuning? Fine-tune Embedding models for Retrieval Augmented Generation (RAG)
description: Finetune, Embedding Model, RAG
categories: [LLM, RAG]
tags: [Finetune, Embedding Model, RAG]
# author: foDev_jeong
date: 2024-06-05 11:00:00 +0800
pin: true
# mermaid: true
# render_with_liquid: false
# image:
#   path: /assets/img/blog/NLP_Overview.svg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [Overview of NLP Course]
---

## RAG or Fine Tuning? A simple feature comparision to decide which technique you should use!

For customizing LLMs, in addition to RAG, another optimization technique is fine-tuning.

> **ğ—¥ğ—”ğ—š** is akin to providing a textbook to the model, allowing it to retrieve information based on specific queries. This approach is suitable for scenarios where the model needs to address particular information retrieval tasks. However, RAG is not suitable for teaching the model to understand broad domains or learn new languages, formats, or styles.
{: .prompt-info }

> **ğ—™ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´** is similar to enabling students to internalize knowledge through extensive learning. Fine-tuning can enhance the performance of non-fine-tuned models and make interactions more efficient. It is particularly suitable for emphasizing existing knowledge in the base model, modifying or customizing the modelâ€™s output, and providing complex directives to the model. 
{: .prompt-info }

Sometimes it may not seem straightforward to choose one approach or the other, that's why this guide will help you to differentiate which technique fits better your use case!

![ Finetuning or RAG ? ](/assets/img/llm/LLM_fintuning_RAG.jpeg){: .light .shadow .rounded-10 w='1212' h='668' }

## RAG in Production: The importance of a Solid Data Strategy ğŸ’¥

Retrieval-Augmented Generation (RAG) has become one of the hottest topics in Generative AI, providing powerful ways to enhance model responses with real-world data. But letâ€™s be honest, without a solid data strategy, youâ€™re setting yourself up for a meme-worthy fail. ğŸ˜‚

ğŸ“ˆ ğ—ªğ—µğ˜† ğ—¥ğ—”ğ—š ğ—¡ğ—²ğ—²ğ—±ğ˜€ ğ—® ğ——ğ—®ğ˜ğ—® ğ—¦ğ˜ğ—¿ğ—®ğ˜ğ—²ğ—´ğ˜†: 

1. ğ——ğ—®ğ˜ğ—® ğ—¤ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜†: Garbage in, garbage out. Your model is only as good as the data it retrieves.
2. ğ—¥ğ—²ğ—¹ğ—²ğ˜ƒğ—®ğ—»ğ—°ğ—²: Ensure your data is relevant to your use case.
3. ğ—¦ğ—°ğ—®ğ—¹ğ—®ğ—¯ğ—¶ğ—¹ğ—¶ğ˜ğ˜†: Manage and scale your data efficiently to keep up with growing demands.

Remember, a well-thought-out data strategy is the backbone of any successful RAG implementation.

ğŸš€ ğ—–ğ—¼ğ—»ğ—°ğ—¹ğ˜‚ğ˜€ğ—¶ğ—¼ğ—»: Donâ€™t let your RAG use case fall flat. Invest in your data strategy and watch your AI soar! ğŸŒŸ

## Fine-tuning can significantly boost retrieval. ğŸ‘€

Embedding models are crucial for Retrieval-Augmented Generation (RAG) applications, but general models often fall short of domain-specific tasks. 

Excited to share a new blog on how to fine-tune embedding models for financial RAG applications using NVIDIA's 2023 SEC Filing dataset using latest research, like Matryoshka Representation Learning:

- ğŸš€ Fine-tuning boosts performance between 7.4% to 22.55% with just 6.3k samples
- âœ… Baseline creation + evaluation during training
- ğŸ§¬ Synthetic data generated used for fine-tuning
- â±ï¸ Training on ~10,000 only 5 minutes on consumer-grade GPUs
- ğŸª† Matryoshka keeps 99% performance at 6x smaller size
- ğŸ“ˆ Fine-tuned 128-dim model outperforms baseline 768-dim by 6.51%
- ğŸ†• Uses the new Sentence Transformers v3


ğŸ‘‰ Original Article : <https://www.philschmid.de/fine-tune-embedding-model-for-rag>

ğŸ‘‰ Code : <https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-embedding-model-for-rag.ipynb>

Go build! ğŸ¤—

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

## RAG ë˜ëŠ” ë¯¸ì„¸ ì¡°ì •? ì–´ë–¤ ê¸°ìˆ ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ê¸°ëŠ¥ ë¹„êµ!

LLMì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ê¸° ìœ„í•´ RAG ì™¸ì—ë„ ë˜ ë‹¤ë¥¸ ìµœì í™” ê¸°ìˆ ì´ ë¯¸ì„¸ ì¡°ì •ì…ë‹ˆë‹¤.

> **RAGëŠ”** ëª¨ë¸ì— êµê³¼ì„œë¥¼ ì œê³µí•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•˜ì—¬ íŠ¹ì • ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª¨ë¸ì´ íŠ¹ì • ì •ë³´ ê²€ìƒ‰ ì‘ì—…ì„ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì— ì í•©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ RAGëŠ” ëª¨ë¸ì´ ê´‘ë²”ìœ„í•œ ë„ë©”ì¸ì„ ì´í•´í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì–¸ì–´, í˜•ì‹ ë˜ëŠ” ìŠ¤íƒ€ì¼ì„ í•™ìŠµí•˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ë°ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
{: .prompt-info }

> **ë¯¸ì„¸ ì¡°ì •ì€** í•™ìƒë“¤ì´ ê´‘ë²”ìœ„í•œ í•™ìŠµì„ í†µí•´ ì§€ì‹ì„ ë‚´ë©´í™”í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ë¯¸ì„¸ ì¡°ì •ì€ ë¯¸ì„¸ ì¡°ì •ë˜ì§€ ì•Šì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ìƒí˜¸ ì‘ìš©ì„ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì˜ ê¸°ì¡´ ì§€ì‹ì„ ê°•ì¡°í•˜ê³ , ëª¨ë¸ì˜ ì¶œë ¥ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ì‚¬ìš©ì ì§€ì •í•˜ê³ , ëª¨ë¸ì— ë³µì¡í•œ ì§€ì‹œë¬¸ì„ ì œê³µí•˜ëŠ” ë° íŠ¹íˆ ì í•©í•©ë‹ˆë‹¤. 
{: .prompt-info }

ë•Œë¡œëŠ” í•œ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ ë˜ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ê°„ë‹¨í•˜ì§€ ì•Šì€ ê²ƒì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ ê°€ì´ë“œëŠ” ì‚¬ìš© ì‚¬ë¡€ì— ë” ì í•©í•œ ê¸°ìˆ ì„ êµ¬ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤!

## ìƒì‚° í˜„ì¥ì—ì„œì˜ RAG: ê²¬ê³ í•œ ë°ì´í„° ì „ëµğŸ’¥ì˜ ì¤‘ìš”ì„±

RAG(Retrieval-Augmented Generation)ëŠ” ì œë„ˆë ˆì´í‹°ë¸Œ AIì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì£¼ì œ ì¤‘ í•˜ë‚˜ê°€ ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ ë°ì´í„°ë¡œ ëª¨ë¸ ì‘ë‹µì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì†”ì§íˆ ë§í•´ì„œ ê²¬ê³ í•œ ë°ì´í„° ì „ëµì´ ì—†ìœ¼ë©´ ë°ˆì— ì–´ìš¸ë¦¬ëŠ” ì‹¤íŒ¨ë¥¼ ë§ì´í•˜ê²Œ ë©ë‹ˆë‹¤. ğŸ˜‚

ğŸ“ˆ RAGì— ë°ì´í„° ì „ëµì´ í•„ìš”í•œ ì´ìœ :

1. ë°ì´í„° í’ˆì§ˆ: ì“°ë ˆê¸° ìœ ì…, ì“°ë ˆê¸° ë°°ì¶œ. ëª¨ë¸ì€ ê²€ìƒ‰í•˜ëŠ” ë°ì´í„°ë§Œí¼ë§Œ ìš°ìˆ˜í•©ë‹ˆë‹¤.
2. ê´€ë ¨ì„±: ë°ì´í„°ê°€ ì‚¬ìš© ì‚¬ë¡€ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
3. í™•ì¥ì„±: ì¦ê°€í•˜ëŠ” ìˆ˜ìš”ë¥¼ ë”°ë¼ì¡ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  í™•ì¥í•©ë‹ˆë‹¤.

ì‹ ì¤‘í•œ ë°ì´í„° ì „ëµì€ ì„±ê³µì ì¸ RAG êµ¬í˜„ì˜ ì¤‘ì¶”ë¼ëŠ” ì ì„ ê¸°ì–µí•˜ì‹­ì‹œì˜¤.

ğŸš€ ê²°ë¡ : RAG ì‚¬ìš© ì‚¬ë¡€ê°€ ì‹¤íŒ¨í•˜ì§€ ì•Šë„ë¡ í•˜ì‹­ì‹œì˜¤. ë°ì´í„° ì „ëµì— íˆ¬ìí•˜ê³  AIê°€ ê¸‰ì¦í•˜ëŠ” ê²ƒì„ ì§€ì¼œë³´ì‹­ì‹œì˜¤! ğŸŒŸ

##  ë¯¸ì„¸ ì¡°ì •ì€ ê²€ìƒ‰ ì†ë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ‘€

ì„ë² ë”© ëª¨ë¸ì€ RAG(Retrieval-Augmented Generation) ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë§¤ìš° ì¤‘ìš”í•˜ì§€ë§Œ ì¼ë°˜ ëª¨ë¸ì€ ë„ë©”ì¸ë³„ ì‘ì—…ì— ë¯¸ì¹˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. 

Matryoshka Representation Learningê³¼ ê°™ì€ ìµœì‹  ì—°êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ NVIDIAì˜ 2023 SEC Filing ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸ˆìœµ RAG ì• í”Œë¦¬ì¼€ì´ì…˜ìš© ì„ë² ë”© ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìƒˆë¡œìš´ ë¸”ë¡œê·¸ë¥¼ ê³µìœ í•˜ê²Œ ë˜ì–´ ê¸°ì©ë‹ˆë‹¤.

- ğŸš€ ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ë‹¨ 6.3k ìƒ˜í”Œë¡œ 7.4%ì—ì„œ 22.55%ê¹Œì§€ ì„±ëŠ¥ í–¥ìƒ
- âœ… ê¸°ì¤€ ìƒì„± + í•™ìŠµ ì¤‘ í‰ê°€
- ğŸ§¬ ë¯¸ì„¸ ì¡°ì •ì— ì‚¬ìš©ë˜ëŠ” ìƒì„±ëœ í•©ì„± ë°ì´í„°
- â±ï¸ ~10,000ì— ëŒ€í•œ êµìœ¡, ì†Œë¹„ììš© GPUì—ì„œ ë‹¨ 5ë¶„
- ğŸª† MatryoshkaëŠ” 6ë°° ë” ì‘ì€ í¬ê¸°ë¡œ 99%ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- ğŸ“ˆ ë¯¸ì„¸ ì¡°ì •ëœ 128-dim ëª¨ë¸ì€ ê¸°ì¤€ 768-dimë³´ë‹¤ 6.51% ë” ìš°ìˆ˜í•©ë‹ˆë‹¤.
- ğŸ†• ìƒˆë¡œìš´ ë¬¸ì¥ ë³€í™˜ê¸° v3 ì‚¬ìš©

ë¹Œë“œí•˜ëŸ¬ ê°€ì„¸ìš”! ğŸ¤—

</details>