---
title: Data Pipelines Overview
description: Script, Datapipeline
categories: [Script, Datapipeline]
tags: [Datapipeline, Cookbook]
# author: foDev_jeong
date: 2024-04-17 15:10:00 +0800
# pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/news/data-pipeline-overview.gif)
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [ Data pipeline Overview ]
---

![Data pipeline Overview ](/assets/img/news/data-pipeline-overview.gif){: .light .w-75 .shadow .rounded-10 w='1212' h='668' }

### Data Pipelines Overview

Data pipelines are a fundamental component of managing and processing data efficiently within modern systems. These pipelines typically encompass 5 predominant phases: Collect, Ingest, Store, Compute, and Consume.

1. Collect:
   - Data is acquired from data stores, data streams, and applications, sourced remotely from devices, applications, or business systems.

2. Ingest:
   - During the ingestion process, data is loaded into systems and organized within event queues.

3. Store:
   - Post ingestion, organized data is stored in data warehouses, data lakes, and data lakehouses, along with various systems like databases, ensuring post-ingestion storage.

4. Compute:
   - Data undergoes aggregation, cleansing, and manipulation to conform to company standards, including tasks such as format conversion, data compression, and partitioning. This phase employs both batch and stream processing techniques.

5. Consume:
   - Processed data is made available for consumption through analytics and visualization tools, operational data stores, decision engines, user-facing applications, dashboards, data science, machine learning services, business intelligence, and self-service analytics.

The efficiency and effectiveness of each phase contribute to the overall success of data-driven operations within an organization.

Over to you: What's your story with data-driven pipelines? How have they influenced your data management game?

–
Subscribe to our weekly newsletter to get a Free System Design PDF (158 pages): <https://bit.ly/3KCnWXq>

* * * 

데이터 파이프라인 개요

데이터 파이프라인은 최신 시스템 내에서 데이터를 효율적으로 관리하고 처리하기 위한 기본 구성 요소입니다. 이러한 파이프라인은 일반적으로 수집, 수집, 저장, 컴퓨팅 및 사용의 5가지 주요 단계를 포함합니다.

1. 수집:
   - 데이터는 데이터 저장소, 데이터 스트림 및 응용 프로그램에서 수집되며 장치, 응용 프로그램 또는 비즈니스 시스템에서 원격으로 제공됩니다.

2. 섭취:
   - 수집 프로세스 중에 데이터가 시스템에 로드되고 이벤트 큐 내에서 구성됩니다.

3. 저장:
   - 수집 후 정리된 데이터는 데이터베이스와 같은 다양한 시스템과 함께 데이터 웨어하우스, 데이터 레이크 및 데이터 레이크하우스에 저장되어 수집 후 저장을 보장합니다.

4. 계산:
   - 데이터는 형식 변환, 데이터 압축 및 분할과 같은 작업을 포함하여 회사 표준을 준수하기 위해 집계, 정리 및 조작을 거칩니다. 이 단계에서는 일괄 처리 및 스트림 처리 기술을 모두 사용합니다.

5. 소비:
   - 처리된 데이터는 분석 및 시각화 도구, 운영 데이터 저장소, 의사 결정 엔진, 사용자 대면 애플리케이션, 대시보드, 데이터 과학, 기계 학습 서비스, 비즈니스 인텔리전스 및 셀프 서비스 분석을 통해 사용할 수 있습니다.

각 단계의 효율성과 효과는 조직 내 데이터 기반 운영의 전반적인 성공에 기여합니다.

데이터 기반 파이프라인에 대한 이야기는 무엇인가요? 데이터 관리 게임에 어떤 영향을 미쳤습니까?

–
주간 뉴스레터를 구독하여 무료 시스템 설계 PDF(158페이지)를 받으십시오. https://bit.ly/3KCnWXq
