---
title: Reinforcing Reinforcement Learning Terms, Policies, Models and Top 40 Libraries ğŸ“š
description: ReinforecementLearning, Library
categories: [Study, ReinforcementLearning]
tags: [Course, ReinforementLearning]
# author: foDev_jeong
date: 2024-07-05 20:10:00 +0800
# pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---

RL is a type of machine learning that lets an agent interact with an environment, receive feedback, and make better decisions over time.

â€”â€”â€”â€”â€”â€”â€”â€”â€”

## ğŸ“ Terms Used in RL:

- âŒ˜ Environment: the system or situation that the agent interacts with. 

- âŒ˜ Agent: refers to an autonomous entity that interacts with an environment.

- âŒ˜ Feedback: refers to the information provided by the environment to the agent after the agent has taken an action (rewards or penalties).

- âŒ˜ State (S): Current situation returned by the environment.

- âŒ˜ Policy(Ï€): The strategy that the agent employs to determine next action.

- âŒ˜ Value (V): The expected long-term return 

- âŒ˜ Q-Value (Q): the long-term return of given current action 

- âŒ˜ Model: stands for the simulation of environment. 

â€”â€”â€”â€”â€”â€”â€”â€”â€”

## ğŸ“– Model/Policy of RL:

Model-Free vs Model-Based:

- à¹ Model-based works with state space and action space grows
- à¹ Model-free algorithms rely on trial-and-error to update its knowledge.

On-Policy vs Off-Policy:

- à¹ On-policy agent learns based on its current action a derived from the current policy, 
- à¹ Off-policy counter part learns it based on another policy.

â€”â€”â€”â€”â€”â€”â€”â€”â€”

## ğŸ¤– Well-known RL Models:

- âŠ Q-Learning:

Model-free algorithm that uses a Q-table to store best action for a state.

- â‹ State-Action-Reward-State-Action (SARSA):

Model-based algorithm that updates state-action value based on reward and next state-action.

- âŒ Deep Q Network (DQN):

Model-free algorithm that uses deep neural networks to approximate the Q-function.

- â Deep Deterministic Policy Gradient (DDPG):

By using deep neural networks, DDPG handles more complex environments and large state spaces than traditional RL algorithms. 

â€”â€”â€”â€”â€”â€”â€”â€”â€”

## ğŸ› ï¸ Here are some applications of reinforcement learning (RL):

- Â» Robotics
- Â» Autonomous Vehicles
- Â» Healthcare
- Â» Finance
- Â» Gaming
- Â» Energy Management
- Â» Marketing and Advertising
- Â» Natural Language Processing
- Â» Manufacturing
- Â» Smart Grids
- Â» Supply Chain Optimization
- Â» Recommendation Systems
- Â» Personalization Systems
- Â» Traffic Signal Control
- Â» Education and Training
- Â» Agriculture
- Â» Industrial Automation
- Â» Space Exploration
- Â» Cybersecurity
- Â» Virtual Assistants

â€”â€”â€”â€”â€”â€”â€”â€”â€”

These are 40 Python Libraries I found for Reinforcement Learning:

- ğŸ“š Gym
- ğŸ“š Baselines
- ğŸ“š Dopamine
- ğŸ“š TensorLayer
- ğŸ“š FinRL
- ğŸ“š Stable-Baselines
- ğŸ“š ReAgent
- ğŸ“š Acme
- ğŸ“š PARL
- ğŸ“š TF-Agents
- ğŸ“š TensorFlow
- ğŸ“š PyTorchRL
- ğŸ“š Keras-RL
- ğŸ“š Garage
- ğŸ“š TensorForce
- ğŸ“š RLax
- ğŸ“š Coach
- ğŸ“š RFRL
- ğŸ“š Rliable
- ğŸ“š ViZDoom
- ğŸ“š Ray RLlib
- ğŸ“š Dopamine
- ğŸ“š Acme
- ğŸ“š Tensorforce
- ğŸ“š ReAgent (Horizon)
- ğŸ“š ChainerRL
- ğŸ“š MushroomRL
- ğŸ“š TRFL
- ğŸ“š CleanRL
- ğŸ“š Tianshou
- ğŸ“š MAgent
- ğŸ“š rl-baselines3-zoo
- ğŸ“š PettingZoo
- ğŸ“š RLlib
- ğŸ“š RoboRL
- ğŸ“š H-baselines
- ğŸ“š DI-engine

Source in the comments â†“

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

â­† ğ‘ğğ ğ¢ğ¬ğ­ğğ« ğŸğ¨ğ« ğ…ğ«ğğ ğğ§ğ¥ğ¢ğ§ğ ğ‡ğšğ§ğğ¬-ğ¨ğ§ ğƒğšğ­ğš ğ’ğœğ¢ğğ§ğœğ ğ“ğ®ğ­ğ¨ğ«ğ¢ğšğ¥ (ğ„ğ§ğ ğ­ğ¨ ğ„ğ§ğ ğğ«ğ¨ğ£ğğœğ­): <https://www.maryammiradi.com/sonar>

![ Reinforcement Learning Python Libraries ](/assets/img/blog/ReinforcementLearning_Library.jpeg){: .light .shadow .rounded-10 w='1212' h='668' }

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

RLì€ ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ ì‘ìš©í•˜ê³ , í”¼ë“œë°±ì„ ë°›ê³ , ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë” ë‚˜ì€ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ê³„ í•™ìŠµì˜ í•œ ìœ í˜•ì…ë‹ˆë‹¤.

â€”â€”â€”â€”â€”â€”â€”â€”â€”

## ğŸ“ RLì—ì„œ ì‚¬ìš©ë˜ëŠ” ìš©ì–´:

- âŒ˜ í™˜ê²½: ì—ì´ì „íŠ¸ê°€ ìƒí˜¸ ì‘ìš©í•˜ëŠ” ì‹œìŠ¤í…œ ë˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤. 

- âŒ˜ ì—ì´ì „íŠ¸(Agent): í™˜ê²½ê³¼ ìƒí˜¸ ì‘ìš©í•˜ëŠ” ììœ¨ì ì¸ ê°œì²´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

- âŒ˜ í”¼ë“œë°±: ì—ì´ì „íŠ¸ê°€ ì¡°ì¹˜(ë³´ìƒ ë˜ëŠ” í˜ë„í‹°)ë¥¼ ì·¨í•œ í›„ í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ì—ê²Œ ì œê³µí•˜ëŠ” ì •ë³´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

- âŒ˜ ìƒíƒœ(S): í™˜ê²½ì—ì„œ ë°˜í™˜ë˜ëŠ” í˜„ì¬ ìƒí™©ì…ë‹ˆë‹¤.

- âŒ˜ ì •ì±…(Ï€): ì—ì´ì „íŠ¸ê°€ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.

- âŒ˜ ê°€ì¹˜ (V) : ì˜ˆìƒë˜ëŠ” ì¥ê¸° ìˆ˜ìµ 

- âŒ˜ Q-Value (Q): ì£¼ì–´ì§„ í˜„ì¬ í–‰ë™ì˜ ì¥ê¸° ìˆ˜ìµ 

- âŒ˜ ëª¨ë¸: í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 

â€”â€”â€”â€”â€”â€”â€”â€”â€”

## ğŸ“– RLì˜ ëª¨ë¸/ì •ì±…:

ëª¨ë¸ í”„ë¦¬(Model-Free) vs ëª¨ë¸ ê¸°ë°˜(Model-Based):

- à¹ ìƒíƒœ ê³µê°„ê³¼ ì•¡ì…˜ ê³µê°„ì´ ì»¤ì§€ëŠ” ëª¨ë¸ ê¸°ë°˜ ì‘í’ˆ
- à¹ Model-free ì•Œê³ ë¦¬ì¦˜ì€ ì§€ì‹ì„ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ì‹œí–‰ì°©ì˜¤ì— ì˜ì¡´í•©ë‹ˆë‹¤.

ì˜¨-í´ë¦¬ì‹œ(On-Policy) vs ì˜¤í”„-í´ë¦¬ì‹œ(Off-Policy):

- à¹ On-policy ì—ì´ì „íŠ¸ëŠ” í˜„ì¬ ì •ì±…ì—ì„œ íŒŒìƒëœ í˜„ì¬ ì‘ì—…ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. 
- à¹ Off-policy ì¹´ìš´í„° íŒŒíŠ¸ëŠ” ë‹¤ë¥¸ ì •ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

â€”â€”â€”â€”â€”â€”â€”â€”â€”

## ğŸ¤– ì˜ ì•Œë ¤ì§„ RL ëª¨ë¸:

- âŠ Q-ëŸ¬ë‹:

Q-í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ìƒíƒœì— ëŒ€í•œ ìµœìƒì˜ ì‘ì—…ì„ ì €ì¥í•˜ëŠ” ëª¨ë¸ ì—†ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

- â‹ êµ­ê°€-í–‰ë™-ë³´ìƒ-êµ­ê°€-í–‰ë™(SARSA):

ë³´ìƒ ë° ë‹¤ìŒ ìƒíƒœ-í–‰ë™ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒíƒœ-í–‰ë™ ê°’ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ëª¨ë¸ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

- âŒ ë”¥ Q ë„¤íŠ¸ì›Œí¬(DQN):

ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ Q-functionì„ ê·¼ì‚¬í™”í•˜ëŠ” ëª¨ë¸ ì—†ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

- â ì‹¬ì¸µ ê²°ì •ë¡ ì  ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸(DDPG):

DDPGëŠ” ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ RL ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë” ë³µì¡í•œ í™˜ê²½ê³¼ ëŒ€ê·œëª¨ ìƒíƒœ ê³µê°„ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. 

â€”â€”â€”â€”â€”â€”â€”â€”â€”

## ğŸ› ï¸ ë‹¤ìŒì€ ê°•í™” í•™ìŠµ(RL)ì˜ ëª‡ ê°€ì§€ ì‘ìš© ë¶„ì•¼ì…ë‹ˆë‹¤.

- Â» ë¡œë³´í‹±ìŠ¤
- Â» ììœ¨ ì£¼í–‰ ì°¨ëŸ‰
- Â» í—¬ìŠ¤ì¼€ì–´
- Â» ê¸ˆìœµ
- Â» ë…¸ë¦„
- Â» ì—ë„ˆì§€ ê´€ë¦¬
- Â» ë§ˆì¼€íŒ… ë° ê´‘ê³ 
- Â» ìì—°ì–´ ì²˜ë¦¬
- Â» ì œì¡°ì—…
- Â» ìŠ¤ë§ˆíŠ¸ ê·¸ë¦¬ë“œ
- Â» ê³µê¸‰ë§ ìµœì í™”
- Â» ì¶”ì²œ ì‹œìŠ¤í…œ
- Â» ê°œì¸í™” ì‹œìŠ¤í…œ
- Â» êµí†µ ì‹ í˜¸ ì œì–´
- Â» êµìœ¡ ë° í›ˆë ¨
- Â» ë†ì—…
- Â» ì‚°ì—… ìë™í™”
- Â» ìš°ì£¼ íƒì‚¬
- Â» ì‚¬ì´ë²„ ë³´ì•ˆ
- Â» ê°€ìƒ ë¹„ì„œ

</details>