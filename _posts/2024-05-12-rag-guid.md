---
title: What is a RAG?
description: RAG, Concept, Simple Equation
categories:
- RAG & Retrieval Systems
- Development & Tools
tags:
- rag
- llm
- concept
date: 2024-05-12 22:39:00 +0800
mermaid: true
---
## RAG = Query + Prompt + Context + LLM

*Curiosity:* What is a RAG? How can we retrieve knowledge from external sources and augment LLM responses to create more accurate, context-aware answers?

**Retrieval-Augmented Generation (RAG)** is a powerful technique that combines the best of information retrieval and language generation. This simple equation captures the essence of RAG: **Query + Prompt + Context + LLM = Enhanced Response**.

### Understanding the RAG Equation

```mermaid
graph TB
    A[User Query] --> B[Query Processing]
    B --> C[Vector Search]
    C --> D[Retrieve Relevant Context]
    D --> E[Augment Prompt]
    E --> F[LLM Generation]
    F --> G[Enhanced Response]
    
    H[Knowledge Base] --> C
    I[Embedding Model] --> C
    
    style A fill:#e1f5ff
    style D fill:#fff3cd
    style F fill:#d4edda
    style G fill:#f8d7da
```

### RAG Components Breakdown

| Component | Purpose | Key Technologies |
|:----------|:--------|:-----------------|
| **Query** | User's information need | Natural language processing, query expansion |
| **Prompt** | Instructions for LLM | System prompts, few-shot examples, templates |
| **Context** | Retrieved relevant information | Vector databases, embedding models, similarity search |
| **LLM** | Text generation | GPT-4, Claude, Llama, Mistral |

### Core RAG Workflow

The RAG process consists of several critical steps:

#### 1. **Indexing Phase** (Offline)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# Step 1: Load documents
documents = load_documents("knowledge_base/")

# Step 2: Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# Step 3: Create embeddings
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings
)
```

#### 2. **Retrieval Phase** (Online)

```python
# Step 4: Retrieve relevant context
query = "What is RAG?"
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
relevant_docs = retriever.get_relevant_documents(query)
```

#### 3. **Augmentation Phase**

```python
# Step 5: Augment prompt with context
context = "\n\n".join([doc.page_content for doc in relevant_docs])
augmented_prompt = f"""Use the following context to answer the question.

Context:
{context}

Question: {query}

Answer:"""
```

#### 4. **Generation Phase**

```python
# Step 6: Generate response
from langchain.llms import OpenAI

llm = OpenAI(temperature=0)
response = llm(augmented_prompt)
print(response)
```

### Key Concepts in RAG

#### Indexing
- **Purpose**: Prepare knowledge base for efficient retrieval
- **Process**: Document loading â†’ Chunking â†’ Embedding â†’ Storage
- **Tools**: LangChain, LlamaIndex, Haystack

#### Loading
- **Sources**: PDFs, web pages, databases, APIs
- **Formats**: Text, markdown, structured data
- **Considerations**: Data quality, format consistency

#### Splitting
- **Strategies**: Character-based, sentence-based, semantic
- **Chunk Size**: Balance between context and precision (typically 200-1000 tokens)
- **Overlap**: Maintain context continuity (typically 10-20% overlap)

#### Embedding
- **Models**: OpenAI `text-embedding-ada-002`, Sentence Transformers, Cohere
- **Dimensions**: 384, 768, 1536 (model-dependent)
- **Evaluation**: MTEB Leaderboard, domain-specific benchmarks

#### Storing
- **Vector Databases**: Pinecone, Weaviate, Chroma, Qdrant, SingleStore
- **Features**: Similarity search, filtering, metadata storage
- **Scalability**: Horizontal scaling, distributed search

#### Retrieval
- **Methods**: Dense retrieval, sparse retrieval, hybrid search
- **Metrics**: Cosine similarity, dot product, Euclidean distance
- **Optimization**: Re-ranking, query expansion, multi-query

#### Augmentation
- **Techniques**: Context injection, prompt engineering, few-shot examples
- **Best Practices**: Clear separation of context and query, structured prompts

#### Generation
- **Models**: GPT-4, Claude, Llama, Mistral
- **Parameters**: Temperature, max_tokens, top_p
- **Optimization**: Streaming, caching, parallel generation

### Advanced RAG Techniques

#### Multi-Query Retrieval

```python
from langchain.retrievers.multi_query import MultiQueryRetriever

# Generate multiple query variations
retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=OpenAI()
)

# Retrieve diverse perspectives
docs = retriever.get_relevant_documents(query)
```

#### Contextual Compression

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# Compress retrieved documents to most relevant parts
compressor = LLMChainExtractor.from_llm(OpenAI())
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)
```

### RAG vs. Fine-tuning Comparison

| Aspect | RAG | Fine-tuning |
|:-------|:----|:------------|
| **Knowledge Update** | âœ… Real-time | âŒ Requires retraining |
| **Source Attribution** | âœ… Yes | âŒ No |
| **Cost** | ğŸ’° Lower (API calls) | ğŸ’°ğŸ’° Higher (training) |
| **Domain Adaptation** | âš ï¸ Limited | âœ… Strong |
| **Hallucination** | âš ï¸ Possible | âš ï¸ Possible |
| **Best For** | Dynamic knowledge, citations | Style, format, domain expertise |

### Tools and Resources

| Tool | Purpose | Link |
|:-----|:--------|:-----|
| **SingleStore** | Vector database with SQL interface | [SingleStore](https://www.singlestore.com/) |
| **LlamaIndex** | Data framework for LLM applications | [LlamaIndex](https://www.llamaindex.ai/) |
| **Superlinked** | Vector database API | [Superlinked](https://www.superlinked.com/) |
| **MTEB Leaderboard** | Embedding model benchmarks | [Hugging Face MTEB](https://huggingface.co/spaces/mteb/leaderboard) |
| **LangChain** | LLM application framework | [LangChain](https://www.langchain.com/) |

### Webinar Information

On May 16th, Thursday, I gave a webinar covering these concepts in depth:

{% include embed/youtube.html id='4rt66pmpwZA' %}

**Hosts**: Matt Brown, Akmal Chaudhri, Esq.

The webinar covered:
- Mathematical foundations of RAG
- Practical implementation examples
- Advanced techniques and optimizations
- Real-world use cases and best practices

**RSVP**: <https://by-hand.ai/rag>

### Key Takeaways

*Retrieve:* RAG enables LLMs to access up-to-date, domain-specific information without retraining.

*Innovate:* By combining retrieval and generation, we can build systems that are both knowledgeable and flexible.

*Curiosity â†’ Retrieve â†’ Innovation:* The RAG workflow embodies this cycleâ€”curiosity drives the query, retrieval brings relevant knowledge, and innovation emerges from the augmented generation.

![ What is RAG? ](/assets/img/llm/RAG_concept.gif){: .light .w-75 .shadow .rounded-10 w='1212' h='668' }

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

RAG = ì¿¼ë¦¬ + í”„ë¡¬í”„íŠ¸ + ì»¨í…ìŠ¤íŠ¸ + LLM ~ RSVP ğŸ‘‰ <https://by-hand.ai/rag>

RAGë€ ë¬´ì—‡ì…ë‹ˆê¹Œ? ì´ê²ƒì€ RAGê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•œ ì´í•´ë¥¼ ê°œë…í™”í•˜ê¸° ìœ„í•´ ìƒê°í•´ ë‚¸ ê°„ë‹¨í•œ ë°©ì •ì‹ì…ë‹ˆë‹¤.

5ì›” 16ì¼ ëª©ìš”ì¼ì—ëŠ” ì›¨ë¹„ë‚˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

### RAG ì´ˆë³´ì ê°€ì´ë“œ ###

ì œ RAG ë°©ì •ì‹ì— ëŒ€í•œ ì„¤ëª…ì„ ë“£ê³  ì‹¶ìœ¼ì‹œë©´ ì›¨ë¹„ë‚˜ì— ì°¸ì—¬í•˜ì‹­ì‹œì˜¤. RSVP ë§í¬ëŠ” ìœ„ì— ìˆìŠµë‹ˆë‹¤.

í•´ì‹œíƒœê·¸#rag í•´ì‹œíƒœê·¸#vectordatabases í•´ì‹œíƒœê·¸#aibyhand 

--ê°œë…--

RAG ë°©ì •ì‹ê³¼ ê´€ë ¨ëœ ì£¼ìš” ê°œë…ì„ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

â€¢ì¸ë±ì‹±
â€¢ë¡œë“œ
â€¢ë¶„í• 
â€¢í¬í•¨
â€¢ì €ì¥
â€¢ê²€ìƒ‰
â€¢í™•ëŒ€
â€¢ì„¸ëŒ€

ì‹œê°„ì´ í—ˆë½í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê³ ê¸‰ ê¸°ìˆ ì— ëŒ€í•´ ì´ì•¼ê¸° í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

â€¢ ë‹¤ì¤‘ ì¿¼ë¦¬
â€¢ ìƒí™©ë³„ ì••ì¶•

--ìˆ˜í•™--

í•­ìƒ ê·¸ë ‡ë“¯ì´ ì œ ì›¨ë¹„ë‚˜ì—ì„œ ìˆ˜í•™ì„ ê¸°ëŒ€í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìˆ˜í•™ì€ ì¬ë¯¸ìˆê³ , ì ‘ê·¼í•˜ê¸° ì‰¬ìš°ë©°, ìœ ìš©í•  ê²ƒì´ë¼ê³  ì¥ë‹´í•©ë‹ˆë‹¤.

--ë„êµ¬--

ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ì–‘í•œ ìœ ìš©í•œ ë„êµ¬ì— ëŒ€í•œ ì˜ˆë¥¼ í¬í•¨í•˜ê² ìŠµë‹ˆë‹¤.
SingleStore 
LlamaIndex 
Superlinked ì˜ API 
Hugging Face ì˜ MTEB ë¦¬ë”ë³´ë“œ


</details>