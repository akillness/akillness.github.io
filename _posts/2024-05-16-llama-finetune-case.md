---
title: Llamma Finetune case ì™€ RLHF ì •ë¦¬
description: LoRA vs GPT4, Performance
categories:
- LLM & Language Models
tags:
- llama
- rlhf
- llm
- language-model
date: 2024-05-16 18:22:00 +0800
mermaid: true
image:
  path: /assets/img/llm/Llamma-finetune.jpeg
  lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
  alt:
  - Workflow
---
## Llama 3 8B Fine-Tune Case: Salesforce's Best-in-Class Model

*Curiosity:* How can we fine-tune Llama 3 8B to beat GPT-3.5? What makes Salesforce's approach with Online Iterative RLHF so effective?

**Salesforce's Llama 3 8B fine-tune** is currently the best available, achieving impressive results through Online Iterative RLHF and open-source datasets.

### Performance Highlights

*Retrieve:* Salesforce's Llama 3 8B achievements.

| Achievement | Details | Impact |
|:------------|:--------|:-------|
| **Benchmark Wins** | Beats GPT-3.5 & Mixtral 8x7B (it) | â¬†ï¸ Superior performance |
| **Benchmarks** | MT bench, Chat Arena Hard, Alpaca Eval | â¬†ï¸ Comprehensive evaluation |
| **RLHF Model** | Beats Llama3-8b-it model | â¬†ï¸ Better than base |
| **Open Source** | No GPT-4/human annotations needed | â¬†ï¸ Accessibility |

**Key Features**:
- âœ… Uses Online Iterative RLHF for efficient alignment
- âœ… Trained with open-source datasets
- âœ… Releases SFT, RLHF, and Reward model



* * * 


### RLHF Workflow: From Reward Modeling to Online RLHF

*Retrieve:* Understanding Online Iterative RLHF.

**Online Iterative RLHF** is widely reported to outperform offline RLHF by a large margin. However, existing open-source RLHF projects are largely confined to offline learning.

**This technical report** fills the gap by providing a detailed, reproducible recipe for online iterative RLHF.

### Online RLHF Architecture

*Innovate:* How online iterative RLHF works.

```mermaid
graph TB
    A[Open-Source Datasets] --> B[Preference Model]
    B --> C[Proxy Human Feedback]
    C --> D[Online Iterative RLHF]
    D --> E[SFT]
    E --> F[Iterative RLHF]
    F --> G[SFR-Iterative-DPO-LLaMA-3-8B-R]
    
    style A fill:#e1f5ff
    style D fill:#fff3cd
    style G fill:#d4edda
```

### Key Innovation

*Retrieve:* Proxy preference model approach.

**Challenge**: Online human feedback is infeasible for open-source communities with limited resources.

**Solution**: 
- Construct preference models using diverse open-source datasets
- Use proxy preference model to approximate human feedback
- Enable online iterative RLHF without human annotators

### Performance Results

*Innovate:* Impressive benchmark achievements.

**Model**: SFR-Iterative-DPO-LLaMA-3-8B-R

**Benchmarks**:
- âœ… AlpacaEval-2
- âœ… Arena-Hard
- âœ… MT-Bench
- âœ… HumanEval
- âœ… TruthfulQA

**Key Achievement**: SFT and iterative RLHF achieve SOTA performance with **fully open-source datasets**.

### Resources

*Retrieve:* Available materials.

> **Resources**:
> - **ðŸ“„ Paper**: <https://arxiv.org/abs/2405.07863>
> - **ðŸ“° Blog**: <https://huggingface.co/blog/rlhf>
{: .prompt-tip}

**Available**:
- âœ… Models
- âœ… Curated datasets
- âœ… Step-by-step code guidebooks

### Key Takeaways

*Retrieve:* Salesforce's Llama 3 8B fine-tune demonstrates that Online Iterative RLHF with open-source datasets can achieve SOTA performance, beating GPT-3.5 and other models.

*Innovate:* By using proxy preference models from open-source datasets, you can implement online iterative RLHF without human annotators, achieving state-of-the-art results with fully open-source resources.

*Curiosity â†’ Retrieve â†’ Innovation:* Start with curiosity about RLHF workflows, retrieve insights from Salesforce's approach, and innovate by implementing online iterative RLHF in your fine-tuning pipelines.

**Next Steps**:
- Read the paper
- Explore the blog post
- Study the code guidebooks
- Implement online RLHF


> RLHF Workflow
- paper : <https://arxiv.org/abs/2405.07863>
- blog : <https://huggingface.co/blog/rlhf>
{: .prompt-tip }


<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

### Llama 3 8BëŠ” ì´ì œê¹Œì§€ ë‚˜ì˜¨ ê²ƒ ì¤‘ ìµœê³ ìž…ë‹ˆë‹¤!

> Salesforceì—ì„œ ë¯¸ì„¸ ì¡°ì •í•œ Llama 3 8B
- GPT-3.5 ë° Mixtral 8x7B(it)ë¥¼ MT ë²¤ì¹˜, Chat Arena Hard ë° Alpaca Evalì—ì„œ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- íš¨ìœ¨ì ì¸ ì •ë ¬ì„ ìœ„í•´ ì˜¨ë¼ì¸ ë°˜ë³µ RLHFë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤ (GPT-4 ë˜ëŠ” ì¸ê°„ ì£¼ì„ í•„ìš” ì—†ìŒ).
- SFT, RLHF ë° ë³´ìƒ ëª¨ë¸ì„ ì¶œì‹œí•©ë‹ˆë‹¤.
- RLHF ëª¨ë¸ì€ ë˜í•œ Llama3-8b-it ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
{: .prompt-info }


RLHF ì›Œí¬í”Œë¡œìš°

ë³´ìƒ ëª¨ë¸ë§ì—ì„œ ì˜¨ë¼ì¸ RLHFê¹Œì§€

ì´ ê¸°ìˆ  ë³´ê³ ì„œì—ì„œëŠ” RLHF(Online Iterative Reinforcement Learning from Human Feedback)ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì œì‹œí•˜ë©°, ì´ëŠ” ìµœê·¼ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ë¬¸í—Œì—ì„œ ì˜¤í”„ë¼ì¸ ëª¨ë¸ë³´ë‹¤ í° ì°¨ì´ë¡œ ìš°ìˆ˜í•œ ê²ƒìœ¼ë¡œ ë„ë¦¬ ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ì˜¤í”ˆ ì†ŒìŠ¤ RLHF í”„ë¡œì íŠ¸ëŠ” ì—¬ì „ížˆ ì˜¤í”„ë¼ì¸ í•™ìŠµ í™˜ê²½ì— êµ­í•œë˜ì–´ ìžˆìŠµë‹ˆë‹¤. ì´ ê¸°ìˆ  ë³´ê³ ì„œì—ì„œëŠ” ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ê³  ì˜¨ë¼ì¸ ë°˜ë³µ RLHFë¥¼ ìœ„í•´ ì‰½ê²Œ ìž¬í˜„í•  ìˆ˜ ìžˆëŠ” ìžì„¸í•œ ë ˆì‹œí”¼ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. íŠ¹ížˆ, ì˜¨ë¼ì¸ ì¸ì  í”¼ë“œë°±ì€ ì¼ë°˜ì ìœ¼ë¡œ ì œí•œëœ ë¦¬ì†ŒìŠ¤ë¥¼ ê°€ì§„ ì˜¤í”ˆ ì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì‹¤í˜„ ê°€ëŠ¥í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ í˜¸ë„ ëª¨ë¸ì„ êµ¬ì„±í•˜ê³  êµ¬ì„±ëœ í”„ë¡ì‹œ ì„ í˜¸ë„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¸ê°„ì˜ í”¼ë“œë°±ì„ ê·¼ì‚¬í™”í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì˜¨ë¼ì¸ ë°˜ë³µ RLHFì˜ ì´ë¡ ì  í†µì°°ë ¥ê³¼ ì•Œê³ ë¦¬ì¦˜ ì›ë¦¬ì— ëŒ€í•´ ë…¼ì˜í•œ í›„ ìžì„¸í•œ ì‹¤ì œ êµ¬í˜„ì— ëŒ€í•´ ë…¼ì˜í•©ë‹ˆë‹¤. í›ˆë ¨ëœ LLMì¸ SFR-Iterative-DPO-LLaMA-3-8B-Rì€ AlpacaEval-2, Arena-Hard, MT-Benchë¥¼ í¬í•¨í•œ LLM ì±—ë´‡ ë²¤ì¹˜ë§ˆí¬ì™€ HumanEval ë° TruthfulQAì™€ ê°™ì€ ê¸°íƒ€ í•™ìˆ  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¸ìƒì ì¸ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì§€ë„ ë¯¸ì„¸ ì¡°ì •(SFT) ë° ë°˜ë³µ RLHFê°€ ì™„ì „í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„° ì„¸íŠ¸ë¥¼ í†µí•´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìžˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ëª¨ë¸, ì„ ë³„ëœ ë°ì´í„° ì„¸íŠ¸ ë° í¬ê´„ì ì¸ ë‹¨ê³„ë³„ ì½”ë“œ ê°€ì´ë“œë¶ì„ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ í–ˆìŠµë‹ˆë‹¤.


> RLHF Workflow
- paper : <https://arxiv.org/abs/2405.07863>
- blog : <https://huggingface.co/blog/rlhf>
{: .prompt-tip }

</details>