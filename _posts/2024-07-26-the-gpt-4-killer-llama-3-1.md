---
title: ğ— ğ—®ğ—¿ğ—¸ ğ—­ğ˜‚ğ—°ğ—¸ğ—²ğ—¿ğ—¯ğ—²ğ—¿ğ—´ ğ—·ğ˜‚ğ˜€ğ˜ ğ—®ğ—»ğ—»ğ—¼ğ˜‚ğ—»ğ—°ğ—²ğ—± ğ˜ğ—µğ—² ğ—šğ—£ğ—§-ğŸ° ğ—¸ğ—¶ğ—¹ğ—¹ğ—²ğ—¿, ğ—Ÿğ—¹ğ—®ğ—ºğ—®-ğŸ¯.ğŸ­ ğŸ’¥
description: LLM, Llama3.1
categories: [LLM, Llama3.1]
tags: [Meta, Llama3.1]
# author: foDev_jeong
date: 2024-07-26 10:00:00 +0800
# pin: true
# math: true
mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---



![ Overview of Llama 3.1 ](/assets/img/llm/New-King-Llama-31.jpeg){: .light .shadow .rounded-10 w='1212' h='668' }


> - ğŸ‘‰ Read our announcement blog post: <https://huggingface.co/blog/llama31>
> - ğŸ¤— Model card for the 405B on the Hub: <https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-FP8>
{: .prompt-info}

## Meta's Llama-3.1 

*Curiosity:* Meta's Llama-3.1 patches the 8B and 70B Llama-3 models, already top performers in their weight class, to make them even better + gives us the strongest open-source model ever with the 405B.



## Two main points:

### ğŸ«… ğ—§ğ—µğ—² ğ—»ğ—²ğ˜„ ğ—¸ğ—¶ğ—»ğ—´ ğ—¼ğ—³ ğ—¢ğ—¦ ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€: ğ—Ÿğ—¹ğ—®ğ—ºğ—®-ğŸ¯.ğŸ­-ğŸ°ğŸ¬ğŸ±ğ—• on par or above GPT-4o on many benchmarks. 

If confirmed on further testing, it is officially the first time that an OS model becomes the strongest model overall, on top of all models from anthropic and OpenAI! 

Let me repeat this: ğ˜ğ—µğ—² ğ˜€ğ˜ğ—¿ğ—¼ğ—»ğ—´ğ—²ğ˜€ğ˜ ğ—Ÿğ—Ÿğ—  ğ—²ğ˜ƒğ—²ğ—¿ ğ—°ğ—®ğ—» ğ—¯ğ—² ğ—±ğ—¼ğ˜„ğ—»ğ—¹ğ—¼ğ—®ğ—±ğ—²ğ—± ğ—³ğ—¿ğ—¼ğ—º ğ˜ğ—µğ—² ğ—›ğ˜‚ğ—¯.

### ğŸ“š ğ—§ğ—µğ—² ğŸ´ğ—• ğ—®ğ—»ğ—± ğŸ³ğŸ¬ğ—• ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€ ğ—®ğ—¿ğ—² ğ—²ğ˜…ğ˜ğ—²ğ—»ğ—±ğ—²ğ—± ğ˜ğ—¼ ğŸ­ğŸ®ğŸ´ğ—¸ ğ˜ğ—¼ğ—¸ğ—²ğ—»ğ˜€ ğ—°ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ—¹ğ—²ğ—»ğ—´ğ˜ğ—µ. 

The previous models were limited to 8k tokens, meaning they could process at max as much text as around 15 pages in a Word doc: this was a terrible blocker anytime you need a bit of memory, like for RAG or agent workflows. 

Well, not anymore! âœ… Now we get a much more comfortable 128k context length for all sizes, which is great for most my agentic use-cases.

Both points above are huge and would be newsworthy, dropping these together in a â€œ3.1â€ version is crazy! ğŸ¤¯

## ğ—§ğ—²ğ—°ğ—µğ—»ğ—¶ğ—°ğ—®ğ—¹ ğ—¶ğ—»ğ˜€ğ—¶ğ—´ğ—µğ˜ğ˜€:

### ğŸ«… ğ—” ğ—»ğ—²ğ˜„ ğŸ°ğŸ¬ğŸ±ğ—•, ğ—½ğ—¼ğ˜€ğ˜€ğ—¶ğ—¯ğ—¹ğ˜† ğ˜ğ—µğ—² ğ˜€ğ˜ğ—¿ğ—¼ğ—»ğ—´ğ—²ğ˜€ğ˜ ğ—Ÿğ—Ÿğ—  ğ—²ğ˜ƒğ—²ğ—¿, with 128k context length, 88.6% on MMLU, a crazy 96.8% on GSM8K.

- â¤ The 405B has a FP8 quantized version. FP8 quantization was only applied to the major linear operators of the model, such as the gate and up and down projections for the FFNs (covering 75% of the inference FLOPs).
- â¤ You still need 8xH100 to run it with full context length.


### ğŸ¦£ Improved 8B & 70B models, with a ğ—ºğ˜‚ğ—°ğ—µ ğ—¹ğ—®ğ—¿ğ—´ğ—²ğ—¿ ğ—°ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ˜€ğ—¶ğ˜‡ğ—² ğ—¼ğ—³ ğŸ­ğŸ®ğŸ´ğ—¸ ğ˜ƒğ˜€ ğŸ´ğ—¸ â‡’ ğ˜ğ—µğ—¶ğ˜€ ğ—¶ğ˜€ ğ—® ğ—´ğ—®ğ—ºğ—²-ğ—°ğ—µğ—®ğ—»ğ—´ğ—²ğ—¿ ğ—³ğ—¼ğ—¿ ğ—¥ğ—”ğ—š ğ—®ğ—»ğ—± ğ—”ğ—´ğ—²ğ—»ğ˜ğ˜€.

- ğŸ“š Pretrained on 15T tokens, a more diverse training dataset than Llama-3 to reinforce multilinguality: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.
- ğŸ”“ License: same as Llama-3, and on top of that it allows using the output data from Llama-3.1 for training other models (distillation)
- âœ¨ One new role for the instruct version: on top of System, User, and Assistant, Ipython lets you write the output of a code tool call! This should work really well with Transformers agents ğŸ‰

Thanks a lot to Meta for this release which will make our lives better! ğŸ¤—


