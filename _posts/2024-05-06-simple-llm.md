---
title: Simple LLM app ë§Œë“¤ì–´ë³´ê¸°
description: pytorch, LLM, simple, prompt
categories: [Blogging, Pytorch]
tags: [study, Simple LLM, Pytorch]
author: foDev_jeong
date: 2024-05-06 18:25:00 +0800
mermaid: true
# render_with_liquid: false
---

LLM(Large Language Model) ê³µë¶€í•˜ê¸° ê³„íšë¶€í„° ì‹¤ì œ ì½”ë“œê¹Œì§€ ì‘ì„±í•˜ëŠ” í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•œë‹¤. ë‹¹ê·¼ì—ì„œ LLM ì‚¬ìš©í•˜ê¸° 


# Step-by-Step Guide to Building LLM Apps Basic to Advanced Components Created

```mermaid
 gantt
  title Gant Chart to represent LLM Study
  Simple LLM App ( Prompt + LLM ) :active, a, 2024-05-08, 2w
  Chaining Prompt ( Prompt Chains + LLM ) :crit, b, after a, 2w
  Adding External Knowledge Base-RAG :c, after b, 2w
  Adding Memory to LLMs :crit, d, after c, 2w
  Using External Tool with LLM : e, after d, 2w
  LLMs Making Decision-Agent :crit, f, after e, 2w
  Fine-Tuning LLM : g, after f, 2w
```


- Step.1
  - Simple LLM App ( Prompt + LLM )
- Step.2
  - Chaining Prompt ( Prompt Chains + LLM )
- Step.3
  - Adding External Knowledge Base : RAG
- Step.4
  - Adding Memory to LLMs
- Step.5
  - Using External Tool with LLM
- Step.6
  - LLMs Making Decision : Agent
- Step.7
  - Fine-Tuning LLM


<!-- ```liquid
{% if product.title contains 'Pack' %}
  This product's title contains the word Pack.
{% endif %}
  No title
``` -->



* * *

ğŸ§ LLM Quantization - GPTQ | QAT | AWQ | GGUF | GGML | PTQ

ğŸš© GPTQ

Accurate Post-Training Quantization for Generative Pre-trained Transformers compresses LLMs by reducing weight bit depth, maintaining processing speed : https://lnkd.in/e59teNuZ

ğŸš© AWQ

Activation-aware Weight Quantization focuses on activation levels for weight quantization, enhancing model performance : https://lnkd.in/esTYJ6MG

ğŸš© QAT

Quantization-Aware Training quantizes specific operators to INT8 precision, offering flexibility to tailor quantization parameters to different parts of the network : https://lnkd.in/exwyERJQ

ğŸš© GGML

GGML is a C++ replica of LLM library, supporting multiple LLMs like LLaMA series & Falcon, optimized for CPU performance : https://lnkd.in/ekk6dhB8

ğŸš© GGUF

GPT-Generated unified Format introduced by llama.cpp team in 2023, replacing GGML. It offers unified file structure, *.safetensors to *.gguf conversion support, cross-platform compatibility inference on CPUs, GPUs, and MPUs : https://lnkd.in/e2YVC8dN

ğŸš© PTQ

Post-Training Quantization reduces model parameters' precision post-training, offering reduced memory consumption, faster inference times, and improved energy efficiency.

ğŸš© K-quants 

It adjust bit precision for model weights based on importance, improving efficiency.Examples such as q2_K, q3_K_S, and q3_K_L, each with varying bit widths for different tensors.


* * * 

ìµœê·¼ ê°œì¸ ì—°êµ¬ ì¤‘ Llama2-13B ëª¨ë¸ì„ Full Fine-tuningí•  ê¸°íšŒê°€ ìƒê²¼ìŠµë‹ˆë‹¤. í˜„ì¬ 7Bë¥¼ ë„˜ì–´ì„œëŠ” ëª¨ë¸ë“¤ì„ A100-80GB 1 ëŒ€ë¡œ í•™ìŠµí•˜ê¸°ê°€ ì ì  ë” ì–´ë ¤ì›Œì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ì— ë§ì€ ë¶„ë“¤ì´ ì €ì™€ ê°™ì´ ë¶„ì‚° í•™ìŠµ í™˜ê²½ì— ëŒ€í•œ ì‹¤ì§ˆì ì¸ ì´í•´ê°€ í•„ìš”í•  ë•Œ ê·¸ ì¥ë²½ì„ ë‚®ì¶”ê³ ì "How to Train LLM? - From Data Parallel to Fully Sharded Data Parallel"ë¼ëŠ” ì œëª©ìœ¼ë¡œ ë…¸ì…˜ì— ê¸€ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

í•´ë‹¹ ê¸€ì€ Data Parallel, Distributed Data Parallel, ê·¸ë¦¬ê³  Fully Sharded Data Parallelì— ì´ë¥´ê¸°ê¹Œì§€ LLM í•™ìŠµì—ì„œ í•„ìš”í•œ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì—°êµ¬ì™€ í•™ìˆ  ìë£Œ, ê·¸ë¦¬ê³  ì´ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë“¤ì´ ì‘ì„±í•œ ê¸€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ NCCL operationì´ ë¬´ì—‡ì¸ì§€, ê·¸ë¦¬ê³  ì´ê²ƒì´ ì–´ë–»ê²Œ ê°ê°ì˜ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹ì—ì„œ ì‘ë™í•˜ëŠ”ì§€ë¥¼ ìœ ê¸°ì ìœ¼ë¡œ ë‹¤ë£¨ê³ ì í–ˆìŠµë‹ˆë‹¤.

ì•„ì§ ë¶€ì¡±í•œ ì ì´ ë§ì§€ë§Œ, ì €ì²˜ëŸ¼ Fully Sharded Data Parallelì— ëŒ€í•œ ê¹Šì€ ì´í•´ë¥¼ ì›í•˜ì‹œëŠ” ë¶„ë“¤ì—ê²Œ ë„ì›€ì´ ë  ê±°ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. (DMìœ¼ë¡œ ë§ì€ í”¼ë“œë°± ë¶€íƒë“œë¦½ë‹ˆë‹¤!)

<https://lnkd.in/dgKJjNCh>

ë˜í•œ ë¶€ë¡ìœ¼ë¡œ, HuggingFaceì˜ Accelerateë¥¼ ì‚¬ìš©í•˜ì—¬ A100-80GBì—ì„œ í° ëª¨ë¸(e.g., Llama2-70B)ì„ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œ ë‚´ìš©ë„ ë…¸ì…˜ì— ë³„ë„ë¡œ ì‘ì„±í–ˆìœ¼ë‹ˆ ë§ì€ ì°¸ê³  ë¶€íƒë“œë¦½ë‹ˆë‹¤!

<https://lnkd.in/dWVSqc-5>


# LLM Lingo: Must-Know Terms

## Foundation Model

LLM designed to generate and understand human-like text across a wide range of use-cases

## Transformer

A popular LLM design known for its attention mechanism and parallel processing abilities

## Prompting

Providing carefully crafted inputs to an LLM to generate desired outputs

## Context-Length

Maximum number of input words/tokens an LLM can consider when generating an output.

## Few-Shot Learning

Providing very few examples to an LLM to assist it in performing a specific task.

## Zero-Shot Learning

Providing only task instructions to the LLM relying solely on its preexisting knowledge

## RAG

Retrieval-Augmented Generation. Appending retrieved information to improve LLM response

## Knowledge Base(KB)

Collection of documents from which relevant information is retrieved in RAG

## Vector Database

Stores vector representations of the KB, aiding the retrieval of relevant information in RAG,

## Fine-Tuning

Adapting an LLM to a specific task or domain by further training it on task-specific data.

## Instruction Tuning

Adjusting an LLM's behavior during fine-tuning by providing specific guidelines/directives

## Hallucination

Tendency of LLMs to sometimes generate incorrect or non-factual information.

