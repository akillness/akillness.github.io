---
title: LLM Quantization
description: pytorch, LLM,
categories: [LLM, GPT types ]
tags: [study, GPT,Types, Pytorch]
author: foDev_jeong
date: 2024-05-05 21:51:00 +0800
# mermaid: true
# render_with_liquid: false
---

# 🧐 LLM Quantization - GPTQ | QAT | AWQ | GGUF | GGML | PTQ

🚩 GPTQ

Accurate Post-Training Quantization for Generative Pre-trained Transformers compresses LLMs by reducing weight bit depth, maintaining processing speed : https://lnkd.in/e59teNuZ

🚩 AWQ

Activation-aware Weight Quantization focuses on activation levels for weight quantization, enhancing model performance : https://lnkd.in/esTYJ6MG

🚩 QAT

Quantization-Aware Training quantizes specific operators to INT8 precision, offering flexibility to tailor quantization parameters to different parts of the network : https://lnkd.in/exwyERJQ

🚩 GGML

GGML is a C++ replica of LLM library, supporting multiple LLMs like LLaMA series & Falcon, optimized for CPU performance : https://lnkd.in/ekk6dhB8

🚩 GGUF

GPT-Generated unified Format introduced by llama.cpp team in 2023, replacing GGML. It offers unified file structure, *.safetensors to *.gguf conversion support, cross-platform compatibility inference on CPUs, GPUs, and MPUs : https://lnkd.in/e2YVC8dN

🚩 PTQ

Post-Training Quantization reduces model parameters' precision post-training, offering reduced memory consumption, faster inference times, and improved energy efficiency.

🚩 K-quants 

It adjust bit precision for model weights based on importance, improving efficiency.Examples such as q2_K, q3_K_S, and q3_K_L, each with varying bit widths for different tensors.