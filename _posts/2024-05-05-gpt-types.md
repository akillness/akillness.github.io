---
title: LLM Quantization
description: pytorch, LLM,
categories: [LLM, GPT types ]
tags: [study, GPT,Types, Pytorch]
author: foDev_jeong
date: 2024-05-05 21:51:00 +0800
# mermaid: true
# render_with_liquid: false
---

# ğŸ§ LLM Quantization - GPTQ | QAT | AWQ | GGUF | GGML | PTQ

ğŸš© GPTQ

Accurate Post-Training Quantization for Generative Pre-trained Transformers compresses LLMs by reducing weight bit depth, maintaining processing speed : https://lnkd.in/e59teNuZ

ğŸš© AWQ

Activation-aware Weight Quantization focuses on activation levels for weight quantization, enhancing model performance : https://lnkd.in/esTYJ6MG

ğŸš© QAT

Quantization-Aware Training quantizes specific operators to INT8 precision, offering flexibility to tailor quantization parameters to different parts of the network : https://lnkd.in/exwyERJQ

ğŸš© GGML

GGML is a C++ replica of LLM library, supporting multiple LLMs like LLaMA series & Falcon, optimized for CPU performance : https://lnkd.in/ekk6dhB8

ğŸš© GGUF

GPT-Generated unified Format introduced by llama.cpp team in 2023, replacing GGML. It offers unified file structure, *.safetensors to *.gguf conversion support, cross-platform compatibility inference on CPUs, GPUs, and MPUs : https://lnkd.in/e2YVC8dN

ğŸš© PTQ

Post-Training Quantization reduces model parameters' precision post-training, offering reduced memory consumption, faster inference times, and improved energy efficiency.

ğŸš© K-quants 

It adjust bit precision for model weights based on importance, improving efficiency.Examples such as q2_K, q3_K_S, and q3_K_L, each with varying bit widths for different tensors.