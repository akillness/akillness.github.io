---
title: ğ—¥ğ—¼ğ˜‚ğ˜ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—¾ğ˜‚ğ—²ğ—¿ğ˜† ğ˜ğ—¼ ğ—® ğ˜€ğ—ºğ—®ğ—¹ğ—¹ğ—²ğ—¿ ğ—Ÿğ—Ÿğ—  ğ˜„ğ—µğ—²ğ—» ğ—½ğ—¼ğ˜€ğ˜€ğ—¶ğ—¯ğ—¹ğ—²
description: Chatbot,RouteLLM
categories: [LLM, RouteLLM]
tags: [Evaluation, RouteLLM]
# author: foDev_jeong
date: 2024-07-20 11:30:00 +0800
# pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---


### RouteLLM â‡’ ğ—°ğ˜‚ğ˜ ğŸ±ğŸ¬% ğ—¼ğ—³ ğ—°ğ—¼ğ˜€ğ˜ âœ‚ï¸

The LMSys team maintains the ChatbotArena, which is a great evaluation system based on thousands of matches: when a user submits a query, they receive the answers from two hidden models A and B, and vote between the two. This preference data allows them to create an ELO ranking, which is a great indicator of model strength.

The team has found another great usage of this preference data they gathered: train a router to route user queries to the most appropriate model.

The main idea is that ğ™¢ğ™–ğ™£ğ™® ğ™¦ğ™ªğ™šğ™§ğ™ğ™šğ™¨ ğ™™ğ™¤ ğ™£ğ™¤ğ™© ğ™§ğ™šğ™¦ğ™ªğ™ğ™§ğ™š ğ™– ğ™¨ğ™©ğ™§ğ™¤ğ™£ğ™œ ğ™¢ğ™¤ğ™™ğ™šğ™¡: for instance â€œsummarize this paragraph in 1 sentenceâ€ can be solved very well by a small model like Llama-3-8B, which is orders of magnitude cheaper to run than the usual behemoths. If you manage to selectively route all easy queries to the smaller LLM, you can save a lot on the costs with minimal performance reduction (a queries will be poorly answered due to mis-routing)

So the team set on to train a router that given a query, chooses the most appropriate LLM to answer it, between a strong/expensive one and a weak/cheap one.

ğŸ› ï¸ Create a router between GPT-4 (strong model) and Mixtral-8x7B (small model)

ğŸ”¢ Use preference data from 80k labels
- â†’ Augment this with gold preference data for specific benchmarks
- â†’ Define custom metrics to measure perf gain from routing
- â†’ Test on MT-Bench, GSM8k, and MMLU

ğŸ’¥ Achieve 95% of GPT-4 quality on MT-Bench for over 2x cost reduction

âœ¨ Overhead cost are minimal, even the most expensive routing method introduces an overhead under 0.4% of GPT-4 generation

ğŸ§‚ Grain of salt: MT-Bench is really the benchmark where this method performs best, and introducing â€œgold dataâ€ from the benchmark probably biased results upwards. So the â€œ95% perf for 2x cost reductionâ€ will not be as impressive in a real setting

> - ğ™ğ™šğ™–ğ™™ ğ™©ğ™ğ™š ğ™¥ğ™–ğ™¥ğ™šğ™§ ğ™ğ™šğ™§ğ™š ğŸ‘‰ <https://huggingface.co/papers/2406.18665>
> - ğ˜¾ğ™¤ğ™™ğ™š ğ™§ğ™šğ™¥ğ™¤ ğ™ğ™¨ ğ™ğ™šğ™§ğ™š (already 1.7k stars) ğŸ‘‰ <https://github.com/lm-sys/RouteLLM>
{: .prompt-info}

![ Router ](/assets/img/llm/routeLLM.png){: .light .shadow .rounded-10 w='1212' h='668' }
