---
title: ğ—¥ğ—¼ğ˜‚ğ˜ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—¾ğ˜‚ğ—²ğ—¿ğ˜† ğ˜ğ—¼ ğ—® ğ˜€ğ—ºğ—®ğ—¹ğ—¹ğ—²ğ—¿ ğ—Ÿğ—Ÿğ—  ğ˜„ğ—µğ—²ğ—» ğ—½ğ—¼ğ˜€ğ˜€ğ—¶ğ—¯ğ—¹ğ—²
description: Chatbot,RouteLLM
categories:
- LLM & Language Models
tags:
- evaluation
- routellm
- llm
- language-model
date: 2024-07-20 11:30:00 +0800
mermaid: true
---
### RouteLLM â‡’ ğ—°ğ˜‚ğ˜ ğŸ±ğŸ¬% ğ—¼ğ—³ ğ—°ğ—¼ğ˜€ğ˜ âœ‚ï¸

*Curiosity:* What insights can we retrieve from this? How does this connect to innovation in the field?

*Curiosity:* The LMSys team maintains the ChatbotArena, which is a great evaluation system based on thousands of matches: when a user submits a query, they receive the answers from two hidden models A and B, and vote between the two. This preference data allows them to create an ELO ranking, which is a great indicator of model strength.



The team has found another great usage of this preference data they gathered: train a router to route user queries to the most appropriate model.

The main idea is that ğ™¢ğ™–ğ™£ğ™® ğ™¦ğ™ªğ™šğ™§ğ™ğ™šğ™¨ ğ™™ğ™¤ ğ™£ğ™¤ğ™© ğ™§ğ™šğ™¦ğ™ªğ™ğ™§ğ™š ğ™– ğ™¨ğ™©ğ™§ğ™¤ğ™£ğ™œ ğ™¢ğ™¤ğ™™ğ™šğ™¡: for instance â€œsummarize this paragraph in 1 sentenceâ€ can be solved very well by a small model like Llama-3-8B, which is orders of magnitude cheaper to run than the usual behemoths. If you manage to selectively route all easy queries to the smaller LLM, you can save a lot on the costs with minimal performance reduction (a queries will be poorly answered due to mis-routing)

So the team set on to train a router that given a query, chooses the most appropriate LLM to answer it, between a strong/expensive one and a weak/cheap one.

ğŸ› ï¸ Create a router between GPT-4 (strong model) and Mixtral-8x7B (small model)

ğŸ”¢ Use preference data from 80k labels
- â†’ Augment this with gold preference data for specific benchmarks
- â†’ Define custom metrics to measure perf gain from routing
- â†’ Test on MT-Bench, GSM8k, and MMLU

ğŸ’¥ Achieve 95% of GPT-4 quality on MT-Bench for over 2x cost reduction

âœ¨ Overhead cost are minimal, even the most expensive routing method introduces an overhead under 0.4% of GPT-4 generation

ğŸ§‚ Grain of salt: MT-Bench is really the benchmark where this method performs best, and introducing â€œgold dataâ€ from the benchmark probably biased results upwards. So the â€œ95% perf for 2x cost reductionâ€ will not be as impressive in a real setting

> - ğ™ğ™šğ™–ğ™™ ğ™©ğ™ğ™š ğ™¥ğ™–ğ™¥ğ™šğ™§ ğ™ğ™šğ™§ğ™š ğŸ‘‰ <https://huggingface.co/papers/2406.18665>
> - ğ˜¾ğ™¤ğ™™ğ™š ğ™§ğ™šğ™¥ğ™¤ ğ™ğ™¨ ğ™ğ™šğ™§ğ™š (already 1.7k stars) ğŸ‘‰ <https://github.com/lm-sys/RouteLLM>
{: .prompt-info}

![ Router ](/assets/img/llm/routeLLM.png){: .light .shadow .rounded-10 w='1212' h='668' }
