---
title:  ğ—¨ğ—»ğ˜ƒğ—²ğ—¶ğ—¹ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—² ğ—§ğ—¼ğ—½ ğŸ­ğŸ® ğ— ğ—®ğ—°ğ—µğ—¶ğ—»ğ—² ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ ğ—”ğ—¹ğ—´ğ—¼ğ—¿ğ—¶ğ˜ğ—µğ—ºğ˜€ !
description: ML, Algorithm
categories: [Study, MLAlgorithm]
tags: [ML, Algorithm]
# author: foDev_jeong
date: 2024-06-27 20:10:00 +0800
# pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---

- ğ—Ÿğ—¶ğ—»ğ—²ğ—®ğ—¿ ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—»: A staple for any machine learning enthusiast, linear regression is like drawing a straight line through data points on a graph to predict future values.

- ğ—Ÿğ—¼ğ—´ğ—¶ğ˜€ğ˜ğ—¶ğ—° ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—»: This algorithm helps us categorise data into discrete outcomes â€” it's all about classification, like sorting fruits into apples and oranges.

- ğ——ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ—§ğ—¿ğ—²ğ—²: Imagine playing a game of intelligent '20 Questions' with your data to make decisions and predictions â€” that's your decision tree algorithm.

- ğ—¥ğ—®ğ—»ğ—±ğ—¼ğ—º ğ—™ğ—¼ğ—¿ğ—²ğ˜€ğ˜: By combining multiple decision trees, this algorithm creates a 'forest' that outperforms any single 'tree' in making more accurate guesses.

- ğ—¦ğ˜‚ğ—½ğ—½ğ—¼ğ—¿ğ˜ ğ—©ğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ— ğ—®ğ—°ğ—µğ—¶ğ—»ğ—²ğ˜€ (ğ—¦ğ—©ğ— ğ˜€): SVMs are the strategists of the algorithm world, finding the best boundaries that separate groups of data points.

- ğ—-ğ—¡ğ—²ğ—®ğ—¿ğ—²ğ˜€ğ˜ ğ—¡ğ—²ğ—¶ğ—´ğ—µğ—¯ğ—¼ğ˜‚ğ—¿ğ˜€: Just like looking for the closest friends, this algorithm looks at the 'nearest neighbours' to predict group belonging.

- ğ—šğ—¿ğ—®ğ—±ğ—¶ğ—²ğ—»ğ˜ ğ—•ğ—¼ğ—¼ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ— ğ—®ğ—°ğ—µğ—¶ğ—»ğ—²ğ˜€: Step by step, this algorithm improves decision-making to minimise mistakes â€” it's all about getting smarter over time.

- ğ——ğ—²ğ—²ğ—½ ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´: Delving into the complex neural networks that mimic the human brain, deep learning excels at recognizing patterns and insights from data like images and sounds.

- ğ—£ğ—¿ğ—¶ğ—»ğ—°ğ—¶ğ—½ğ—®ğ—¹ ğ—–ğ—¼ğ—ºğ—½ğ—¼ğ—»ğ—²ğ—»ğ˜ ğ—”ğ—»ğ—®ğ—¹ğ˜†ğ˜€ğ—¶ğ˜€ (ğ—£ğ—–ğ—”): PCA simplifies data by focusing on the most important parts, making it easier to analyse and visualise.

- ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€: Based on probability and assumptions of independence, this algorithm is a quick and dirty way to make predictions.

- ğ—–ğ—¹ğ˜‚ğ˜€ğ˜ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—”ğ—¹ğ—´ğ—¼ğ—¿ğ—¶ğ˜ğ—µğ—º: Ever tried grouping similar things together without being told what the groups should be? That's clustering for you.

- ğ—¡ğ—²ğ˜‚ğ—¿ğ—®ğ—¹ ğ—¡ğ—²ğ˜ğ˜„ğ—¼ğ—¿ğ—¸: The backbone of deep learning, neural networks are inspired by our brain's interconnectivity and are crucial for complex problem-solving.

ğ—¨ğ—»ğ—¹ğ—¼ğ—°ğ—¸ ğ— ğ—¼ğ—¿ğ—² ğ—”ğ—œ ğ—šğ—¼ğ—¼ğ—±ğ—¶ğ—²ğ˜€!
If this content helps, repost this â™»ï¸ to your network and follow Dirk Zee.


![ 3 Day RAG Roadmap ](/assets/img/blog/Best-ML-Algorithm.gif){: .light .shadow .rounded-10 w='1212' h='668' }

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

- ì„ í˜• íšŒê·€: ëª¨ë“  ê¸°ê³„ í•™ìŠµ ì• í˜¸ê°€ì˜ í•„ìˆ˜í’ˆì¸ ì„ í˜• íšŒê·€ëŠ” ê·¸ë˜í”„ì˜ ë°ì´í„° ìš”ì†Œë¥¼ í†µí•´ ì§ì„ ì„ ê·¸ë ¤ ë¯¸ë˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

- ë¡œì§€ìŠ¤í‹± íšŒê·€: ì´ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ë¥¼ ë¶ˆì—°ì†ì ì¸ ê²°ê³¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë©°, ê³¼ì¼ì„ ì‚¬ê³¼ì™€ ì˜¤ë Œì§€ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë¶„ë¥˜ì— ê´€í•œ ê²ƒì…ë‹ˆë‹¤.

- ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬(Decision Tree): ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ê°™ì€ ê²°ì •ì„ ë‚´ë¦¬ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì§€ëŠ¥ì ì¸ '20ê°€ì§€ ì§ˆë¬¸' ê²Œì„ì„ í•œë‹¤ê³  ìƒìƒí•´ ë³´ì‹­ì‹œì˜¤.

- ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest): ì—¬ëŸ¬ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ë¥¼ ê²°í•©í•˜ì—¬ ì´ ì•Œê³ ë¦¬ì¦˜ì€ ë³´ë‹¤ ì •í™•í•œ ì¶”ì¸¡ì„ í•˜ëŠ” ë° ìˆì–´ ë‹¨ì¼ 'íŠ¸ë¦¬'ë¥¼ ëŠ¥ê°€í•˜ëŠ” 'í¬ë ˆìŠ¤íŠ¸'ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

- SVM(Support Vector Machine): SVMì€ ì•Œê³ ë¦¬ì¦˜ ì„¸ê³„ì˜ ì „ëµê°€ë¡œì„œ ë°ì´í„° í¬ì¸íŠ¸ ê·¸ë£¹ì„ êµ¬ë¶„í•˜ëŠ” ìµœìƒì˜ ê²½ê³„ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

- K-Nearest Neighbours: ê°€ì¥ ê°€ê¹Œìš´ ì¹œêµ¬ë¥¼ ì°¾ëŠ” ê²ƒì²˜ëŸ¼ ì´ ì•Œê³ ë¦¬ì¦˜ì€ 'ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ'ì„ ë³´ê³  ê·¸ë£¹ ì†Œì†ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

- ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹ (Gradient Boosting Machines): ì´ ì•Œê³ ë¦¬ì¦˜ì€ ì‹¤ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì˜ì‚¬ ê²°ì •ì„ ë‹¨ê³„ë³„ë¡œ ê°œì„ í•˜ë©°, ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë” ë˜‘ë˜‘í•´ì§€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

- ë”¥ ëŸ¬ë‹: ì¸ê°„ì˜ ë‡Œë¥¼ ëª¨ë°©í•œ ë³µì¡í•œ ì‹ ê²½ë§ì„ íƒêµ¬í•˜ëŠ” ë”¥ ëŸ¬ë‹ì€ ì´ë¯¸ì§€ ë° ì†Œë¦¬ì™€ ê°™ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ê³¼ í†µì°°ë ¥ì„ ì¸ì‹í•˜ëŠ” ë° íƒì›”í•©ë‹ˆë‹¤.

- ì£¼ì„±ë¶„ ë¶„ì„(PCA): PCAëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ì—¬ ë°ì´í„°ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ë¶„ì„ ë° ì‹œê°í™”ë¥¼ ë” ì‰½ê²Œ í•©ë‹ˆë‹¤.

- ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(Naive Bayes): í™•ë¥ ê³¼ ë…ë¦½ì„± ê°€ì •ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì´ ì•Œê³ ë¦¬ì¦˜ì€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ë¹ ë¥´ê³  ì§€ì €ë¶„í•œ ë°©ë²•ì…ë‹ˆë‹¤.

- í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ : ê·¸ë£¹ì´ ë¬´ì—‡ì´ì–´ì•¼í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ì§€ ì•Šê³  ë¹„ìŠ·í•œ ê²ƒë“¤ì„ í•¨ê»˜ ê·¸ë£¹í™”í•˜ë ¤ê³  ì‹œë„í•œ ì ì´ ìˆìŠµë‹ˆê¹Œ? ì´ê²ƒì´ ë°”ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì…ë‹ˆë‹¤.

- ì‹ ê²½ë§: ë”¥ ëŸ¬ë‹ì˜ ì¤‘ì¶”ì¸ ì‹ ê²½ë§ì€ ìš°ë¦¬ ë‡Œì˜ ìƒí˜¸ ì—°ê²°ì„±ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìœ¼ë©° ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

</details>