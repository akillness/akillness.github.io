---
title: All about transformer background knowledge including RNN
description: Transformer, RNN, Knowledge
categories: [Study, Transformer]
tags: [Study, Transformer]
# author: foDev_jeong
date: 2024-06-28 00:10:00 +0800
# pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---

> Update comming soon.. 
{: .prompt-danger}

# Step to explain Transformer 
> Dataset : <https://github.com/NoCodeProgram/deepLearning/tree/main/rnn>


## Transformer Intro

## Self-Attention

## Multi-Head Attention 

## Layer Norm & Encoder

## Embedding Layer

> Test Tokenizer according to Model : <https://tiktokenizer.vercel.app/?model=gpt2>
{: .prompt-tip}

- WordToVec of RNN 

## Positional Encoding Layer

## Transformer Encoder

## Transformer Decoder

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Explaining about Transformer by talker programming without code </summary>

{% include embed/youtube.html id='34gBoeY62zE' %}

</details>