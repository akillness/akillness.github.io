---
title: The release of NVIDIA NIM on Hugging Face Inference Endpoints
description: NVIDIA, EndPoint
categories:
- LLM & Language Models
tags:
- ai
- nvidia
- endpoint
date: 2024-06-04 02:00:00 +0800
mermaid: true
---
## NVIDIA NIM on Hugging Face Inference Endpoints: Simplified AI Deployment

*Curiosity:* How can we simplify generative AI model deployment? What happens when NVIDIA's inference services meet Hugging Face's platform?

**At COMPUTEX**, Jensen Huang announced NVIDIA NIM on Hugging Face Inference Endpoints. NVIDIA NIM are inference services designed to streamline and accelerate generative AI model deployment.

> **Learn More**: <https://developer.nvidia.com/blog/nvidia-collaborates-with-hugging-face-to-simplify-generative-ai-model-deployments>
{: .prompt-info}

### Key Features

*Retrieve:* NVIDIA NIM simplifies AI model deployment.

| Feature | Description | Benefit |
|:--------|:------------|:--------|
| **1-Click Deployment** | From Hugging Face Hub | â¬†ï¸ Ease of use |
| **High Performance** | Up to 9000 tokens/sec | â¬†ï¸ Speed |
| **Cloud Support** | AWS, GCP | â¬†ï¸ Flexibility |
| **Model Variety** | Multiple models supported | â¬†ï¸ Options |

### Available Models

*Retrieve:* Current and upcoming model support.

**Currently Available**:
- Llama 3 8B
- Llama 3 70B

**Coming Soon**:
- Mixtral 8x22B
- Phi-3
- Gemma
- More models

### Performance

*Innovate:* Impressive inference speeds.

**Llama 3 8B**:
- **Up to 9000 tokens/sec**
- High throughput
- Low latency

### Deployment Workflow

*Retrieve:* Simple deployment process.

```mermaid
graph LR
    A[Hugging Face Hub] --> B[1-Click Deploy]
    B --> C[NVIDIA NIM]
    C --> D[Inference Endpoint]
    D --> E[Production Ready]
    
    F[AWS/GCP] --> C
    
    style A fill:#e1f5ff
    style B fill:#fff3cd
    style E fill:#d4edda
```

**Steps**:
1. Select model from Hugging Face Hub
2. One-click deployment
3. Automatic setup on AWS/GCP
4. Production-ready inference endpoint

### Benefits

*Innovate:* Why NVIDIA NIM matters.

**For Developers**:
- âœ… Simplified deployment
- âœ… High performance
- âœ… Cloud flexibility
- âœ… Easy scaling

**For Organizations**:
- âœ… Faster time to production
- âœ… Reduced infrastructure complexity
- âœ… Cost-effective scaling
- âœ… Enterprise-ready

### Key Takeaways

*Retrieve:* NVIDIA NIM on Hugging Face Inference Endpoints enables 1-click deployment of generative AI models with high performance (up to 9000 tokens/sec) on AWS and GCP.

*Innovate:* By combining NVIDIA's inference optimization with Hugging Face's platform, developers can deploy production-ready AI models in minutes instead of days, accelerating AI adoption.

*Curiosity â†’ Retrieve â†’ Innovation:* Start with curiosity about simplified AI deployment, retrieve insights from NVIDIA NIM's capabilities, and innovate by deploying your models with unprecedented ease and performance.

**Next Steps**:
- Explore Hugging Face Inference Endpoints
- Try NVIDIA NIM deployment
- Test performance
- Deploy your models


<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

##  ì–´ì œ COMPUTEXì—ì„œ Jensen Huang ëŠ” Hugging Face ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ì— ëŒ€í•œ NVIDIA NIMì˜ ì¶œì‹œë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤!

ğŸš€ NVIDIA NIMì€ ìƒì„±í˜• AI ëª¨ë¸ì˜ ë°°í¬ë¥¼ ê°„ì†Œí™”í•˜ê³  ê°€ì†í™”í•˜ë„ë¡ ì„¤ê³„ëœ ì¶”ë¡  ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ğŸ‘€

- 1ï¸âƒ£ Hugging Face Hubì—ì„œ Inference Endpointsë¡œ 1-í´ë¦­ ë°°í¬
- ğŸ†• AWSì˜ Llama 3 8B ë° Llama 3 70Bë¶€í„° GCP
- ğŸš€ ìµœëŒ€ 9000 í† í°/ì´ˆ (Llama 3 8B)
- ğŸ”œ Mixtral 8x22B, Phi-3 ë° Gemma ë“±ì´ ê³§ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤.

ìì„¸íˆë³´ê¸°: <https://developer.nvidia.com/blog/nvidia-collaborates-with-hugging-face-to-simplify-generative-ai-model-deployments>

</details>