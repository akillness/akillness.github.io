---
title: "Stop Generating, Start Thinking: The Missing Layer in AI Coding"
description: "A deep reflection on why code generation without human thinking is riskyâ€”and how to build with agents without losing engineering judgment."
categories: [AI, Games]
tags: [Agents, Coding, Workflow, Reliability, Culture]
date: 2026-02-10 13:00:00 +0900
mermaid: true
math: false
image:
  path: /assets/img/2026-02-10-stop-generating/cover.png
  alt: "Stop generating, start thinking"
---

![Stop generating, start thinking](/assets/img/2026-02-10-stop-generating/cover.png)

## ğŸ¤” Curiosity: Are we outsourcing the part that matters most?

I love tools that accelerate development, but Iâ€™m uneasy with one trend: **replacing thinking with generation**. The Localghost essay â€œStop generating, start thinkingâ€ captures that unease sharply. It argues that the most dangerous failure mode of AI coding isnâ€™t bugsâ€”itâ€™s **eroding the act of engineering judgment**.

**Question:** How do we use AI assistants without giving up the responsibility of reasoning about our systems?

---

## ğŸ“š Retrieve: What the essay is really saying

### 1) â€œSpicy autocompleteâ€ is fine. Blind generation isnâ€™t.
The author has used Copilot and Claude as **autocomplete + debugging aids**, but finds that â€œcleverâ€ prompts often cost more time than writing the code directly. The tension isnâ€™t about toolsâ€”itâ€™s about **misplaced effort**.

### 2) The Industrial Revolution analogy is misleading
Yes, this is a productivity revolution. But unlike mechanisationâ€”which produces **consistent, inspectable outputs**â€”LLM outputs are **nonâ€‘deterministic and opaque**. The comparison breaks down when you need accountability.

### 3) LLMs donâ€™t reason; humans must
The essayâ€™s central critique: if humans stop thinking and LLMs canâ€™t think, **nobody is thinking**. This is how brittle systems happen. The Horizon scandal is used as a painful reminder: software failures have **real human cost**.

### 4) â€œHuman centipede epistemologyâ€
We trained models on mediocre code, and now they generate more of itâ€”creating a selfâ€‘reinforcing cycle of lowâ€‘quality output. The author calls this out as a **cultural** problem, not just a technical one.

### 5) Trust is diluted in AIâ€‘generated PRs
A PR written by a teammate carries human intent. A PR written by an agent often doesnâ€™t. When we generate code and let one reviewer approve it, we remove an entire layer of **shared context**.

---

## ğŸ’¡ Innovation: How Iâ€™d operationalize this in real teams

### 1) Split â€œgenerationâ€ from â€œdesign thinkingâ€
I treat agents as **implementation accelerators**, not decisionâ€‘makers.

```mermaid
graph LR
  A[Human Reasoning] --> B[Agent Implementation]
  B --> C[Human Review + Tests]
  C --> A
```

### 2) Require intent in PRs
If AI helped generate code, the PR must still answer:
- Why is this change needed?
- What tradeoffs were considered?
- What tests validate it?

### 3) Use agents for tasks you already understand
This aligns with Mikayla Makiâ€™s advice: **treat agents like an external contributor you donâ€™t fully trust**. If you canâ€™t explain the change, you shouldnâ€™t ship it.

### 4) Keep the pairâ€‘programming effect
AI should behave like a pair, not a replacement. That means **two minds** still need to reasonâ€”one human, one agent, with explicit checks.

---

## Practical guardrails (what Iâ€™d actually enforce)

| Guardrail | Why it matters | Implementation |
|---|---|---|
| â€œReasoned PRâ€ requirement | Forces intent, not just output | PR template with decisions + tests |
| Agent scope limits | Avoids architectural drift | Only allow agents in scoped modules |
| Review debt tracking | AI output still needs human clarity | Track review time per AI PR |
| â€œNo blackâ€‘box mergesâ€ | Prevents blind approvals | Require a second human reviewer |

---

## Where this lands for game development

In games, the temptation to â€œvibe codeâ€ is hugeâ€”rapid prototypes, liveâ€‘ops pressure, short timelines. But games are **systems of constraints** (performance, accessibility, fairness, economy balance). Those constraints require human reasoning.

AI should accelerate the **boring parts**, not replace the **thinking parts**.

---

## Key Takeaways

| Insight | Implication | Next Steps |
|---|---|---|
| LLMs generate, they donâ€™t reason | Humans must own system thinking | Enforce intent in PRs |
| Generation without thinking reduces accountability | Bugs become cultural issues | Build guardrails + reviews |
| AIâ€‘generated PRs erode shared context | Teams lose â€œtwo pairs of eyesâ€ | Pair with explicit reasoning |

### New Questions
- Can we quantify â€œthinking debtâ€ in AIâ€‘generated codebases?
- What would a CI pipeline look like that checks for **intent**, not just tests?
- How do we teach juniors to think if they never write from scratch?

---

## References
- Source essay: https://localghost.dev/blog/stop-generating-start-thinking/
- Humanâ€‘inâ€‘theâ€‘loop take: https://zed.dev/blog/on-programming-with-agents
