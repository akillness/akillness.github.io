---
title: "Harness Engineering: The 5 Rules That Let Agents Ship 1M Lines"
description: "OpenAIâ€™s Codex experiment reframes engineering: not writing code, but designing the harness. Here are the five principles that made it work." 
categories: [AI, Games]
tags: [Agents, Codex, Harness, Workflow, Tooling]
date: 2026-02-11 13:00:00 +0900
mermaid: true
math: false
image:
  path: /assets/img/2026-02-11-harness-engineering/cover.jpg
  alt: "Harness engineering"
---

![Harness engineering](/assets/img/2026-02-11-harness-engineering/cover.jpg)

## ğŸ¤” Curiosity: If agents write all the code, what do engineers do?

OpenAI just described a wild constraint: **no humanâ€‘written code**, only Codex agents. Starting in late Aug 2025, they grew an empty repo into **~1M lines**, shipping a real product with daily usersâ€”**~10Ã— faster** than manual development.

The real takeaway isnâ€™t â€œagents are smart.â€ Itâ€™s that **engineering becomes harness design**: the environment, constraints, and feedback loops that let agents work safely.

---

## ğŸ“š Retrieve: The 5 harness principles (from OpenAI + references)

### 1) If the agent canâ€™t see it, it doesnâ€™t exist
Knowledge buried in Google Docs, Slack, or a human brain is invisible. The team pushed **all decisions into the repo**: markdown, schemas, and execution plans.

- ExecPlans live in **PLANS.md**
- Must be selfâ€‘contained, beginnerâ€‘readable
- Codex runs **7+ hours** off a single plan
- ARCHITECTURE.md is a map, not a manual

### 2) Donâ€™t say â€œtry harderâ€â€”ask â€œwhat capability is missing?â€
When Codex was slow, the fix wasnâ€™t promptsâ€”it was **missing environment affordances**. They asked: what tool, abstraction, or evidence does the agent need to move?

Examples:
- Custom concurrency helper integrated with OpenTelemetry
- â€œBoring techâ€ is often better (stable APIs + clear training signal)

### 3) Consistency comes from mechanical constraints
Docs alone didnâ€™t keep the codebase coherent. The team enforced **invariants** with linters and structure tests.

- Fixed layers per domain (Types â†’ Config â†’ Repo â†’ Service â†’ Runtime â†’ UI)
- Dependency direction validated by CI
- Custom linters (written by Codex) fail builds on violations

### 4) Give agents eyes and observability
They wired **Chrome DevTools Protocol** into the agent runtime to inspect DOM, take screenshots, and compare states. Observability stacks were **ephemeral per worktree**, so each agent had its own logs/metrics.

- Agents query **LogQL** and **PromQL** directly
- Prompts like â€œstartup < 800msâ€ become executable
- Single runs often last **6+ hours**

### 5) Give a map, not a 1,000â€‘page manual
Large monolithic AGENTS.md files fail. Context is scarce. The solution: **short map + structured knowledge base**.

- AGENTS.md = table of contents
- docs/ is the system of record
- Docâ€‘gardening agents keep it fresh
- Architectural invariants often defined by **absence**

---

## ğŸ’¡ Innovation: What this changes for real teams

### 1) Engineering shifts from code to environment
Your main job becomes **constraints + feedback**. If your harness is weak, agents will drift. If itâ€™s strong, they converge.

### 2) Make knowledge â€œrepoâ€‘nativeâ€
If it isnâ€™t in the repo, the agent wonâ€™t learn it. This also mirrors onboarding: new hires can only use whatâ€™s visible.

### 3) Mechanical enforcement beats narrative guidance
Policies that fail the build are more reliable than documentation alone. This is the heart of harness engineering.

---

## A compact harness loop

```mermaid
graph TB
  A[Repo Knowledge] --> B[ExecPlan]
  B --> C[Agent Run]
  C --> D[Tests + Linters]
  D --> E[Observability]
  E --> B
```

---

## Key Takeaways

| Insight | Implication | Next Steps |
|---|---|---|
| Agents need a harness, not hero prompts | Environment design is the new craft | Build constraints + feedback loops |
| Repoâ€‘native knowledge is nonâ€‘optional | Hidden docs donâ€™t exist to agents | Move decisions into markdown + plans |
| Mechanical enforcement scales | Consistency beats micromanagement | Invest in linters + structural tests |

### New Questions
- Can harnesses keep 5â€‘year architectures coherent?
- What â€œminimum viable harnessâ€ works for small teams?
- How do we measure harness quality objectively?

---

## References
- OpenAI blog: https://openai.com/index/harness-engineering/
- ARCHITECTURE.md concept: https://matklad.github.io/2021/02/06/ARCHITECTURE.md.html
- ExecPlans (PLANS.md): https://developers.openai.com/cookbook/articles/codex_exec_plans
- â€œAI is forcing us to write good codeâ€: https://bits.logic.inc/p/ai-is-forcing-us-to-write-good-code
