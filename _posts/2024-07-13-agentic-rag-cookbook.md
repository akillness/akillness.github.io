---
title: ğ€ğ ğğ§ğ­ğ¢ğœ ğ‘ğ€ğ† âœ¨ new cookbook
description: Agentic, RAG, Cookbook
categories: [LLM, Cookbook]
tags: [Agentic, RAG]
# author: foDev_jeong
date: 2024-07-13 14:00:00 +0800
# pin: true
# math: true
mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---

## Agentic RAG Cookbook: Improving RAG with Agent Systems

*Curiosity:* What if RAG systems could think more like humansâ€”questioning their own retrievals, reformulating queries, and iterating until they find the right answer? What happens when we give RAG the ability to retrieve, critique, and retrieve again?

**A new cookbook** demonstrates how to easily improve RAG with an agent system using Transformers Agents. This approach addresses key limitations of vanilla RAG by making systems more intelligent and self-correcting.

### Vanilla RAG Limitations

*Retrieve:* Vanilla RAG systems have fundamental limitations that impact performance.

**Key Limitations**:

| Limitation | Description | Impact |
|:-----------|:------------|:-------|
| **Single Retrieval** | Retrieves documents only once | âš ï¸ Poor quality if initial retrieval fails |
| **Suboptimal Similarity** | Uses user query as reference | âš ï¸ Questions vs. statements mismatch |
| **No Self-Correction** | Cannot refine or re-retrieve | âŒ No improvement mechanism |

**Problem Details**:
- User queries are typically questions
- Relevant documents use affirmative statements
- Similarity scores are downgraded
- No opportunity for improvement

### Vanilla RAG vs. Agentic RAG

| Aspect | Vanilla RAG | Agentic RAG |
|:-------|:------------|:-------------|
| **Retrieval Strategy** | Single retrieval pass | Iterative retrieval with critique |
| **Query Handling** | Direct user query | Query reformulation & optimization |
| **Self-Correction** | âŒ No | âœ… Yes - can re-retrieve if needed |
| **Performance** | Baseline (70.0%) | Improved (+8.5% = 78.5%) |
| **Latency** | Lower (1 LLM call) | Higher (multiple LLM calls) |
| **Quality** | âš ï¸ Limited | â¬†ï¸ Better |

### Agentic RAG Solution

*Innovate:* Making a RAG agentâ€”simply, an agent armed with a retriever toolâ€”alleviates both problems!

**Key Capabilities**:
- âœ… **Query Reformulation**: Agent formulates optimized queries
- âœ… **Self-Query**: Agent critiques content and re-retrieves if needed

**Architecture**:

```mermaid
graph TD
    A[User Query] --> B[Agent: Query Reformulation]
    B --> C[Retrieve Documents]
    C --> D[Agent: Critique Retrieved Content]
    D --> E{Content<br/>Relevant?}
    E -->|No| B
    E -->|Yes| F[Generate Answer]
    F --> G[Final Response]
    
    style B fill:#e1f5ff
    style D fill:#fff3cd
    style F fill:#d4edda
    style E fill:#f8d7da
```

### Performance Comparison

*Retrieve:* Evaluation with LLM-as-a-judge (Llama-3-70B) shows significant improvement.

| Metric | Vanilla RAG | Agentic RAG | Improvement |
|:-------|:------------|:-------------|:------------|
| **Accuracy Score** | 70.0% | 78.5% | **+8.5%** ğŸ’ª |
| **LLM Calls** | 1 | 3-5 | Higher latency |
| **Self-Correction** | âŒ | âœ… | Better quality |
| **Query Optimization** | âŒ | âœ… | Better retrieval |

**Trade-offs**:
- â¬†ï¸ Better quality (+8.5%)
- âš ï¸ Higher latency (multiple LLM calls)
- âš–ï¸ Balance quality vs. speed needed

### Sample Agentic RAG Implementation

```python
from transformers import pipeline
from langchain.agents import create_react_agent
from langchain.tools import Tool

# Create retrieval tool
retrieval_tool = Tool(
    name="retrieve_documents",
    func=vector_store.similarity_search,
    description="Retrieves relevant documents for a query"
)

# Create agent with retrieval tool
agent = create_react_agent(
    llm=llm,
    tools=[retrieval_tool],
    prompt=agent_prompt
)

# Agent workflow
def agentic_rag(query):
    # Step 1: Query reformulation
    reformulated_query = agent.run(
        f"Reformulate this query for better retrieval: {query}"
    )
    
    # Step 2: Retrieve documents
    docs = retrieval_tool.run(reformulated_query)
    
    # Step 3: Critique and potentially re-retrieve
    critique = agent.run(
        f"Critique these documents for relevance to: {query}\n{docs}"
    )
    
    if "not relevant" in critique.lower():
        # Re-retrieve with different strategy
        docs = retrieval_tool.run(query, k=10)  # Get more docs
    
    # Step 4: Generate answer
    answer = llm.generate(
        context=docs,
        question=query
    )
    
    return answer
```

ğ——ğ—¶ğ˜€ğ—°ğ—¼ğ˜ƒğ—²ğ—¿ ğ˜ğ—µğ—² ğ—°ğ—¼ğ—¼ğ—¸ğ—¯ğ—¼ğ—¼ğ—¸ ğŸ‘‡
- <https://huggingface.co/learn/cookbook/agent_rag>


## ğ—”ğ—´ğ—²ğ—»ğ˜ğ—¶ğ—° ğ——ğ—®ğ˜ğ—® ğ—®ğ—»ğ—®ğ—¹ğ˜†ğ˜€ğ˜: ğ—±ğ—¿ğ—¼ğ—½ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—±ğ—®ğ˜ğ—® ğ—³ğ—¶ğ—¹ğ—², ğ—¹ğ—²ğ˜ ğ˜ğ—µğ—² ğ—Ÿğ—Ÿğ—  ğ—±ğ—¼ ğ˜ğ—µğ—² ğ—®ğ—»ğ—®ğ—¹ğ˜†ğ˜€ğ—¶ğ˜€ ğŸ“Šâš™ï¸

Need to make quick exploratory data analysis? â¡ï¸ Get help from an agent.

I was impressed by Llama-3.1's capacity to derive insights from data. Given a csv file, it makes quick work of exploratory data analysis and can derive interesting insights.

On the data from the Kaggle titanic challenge, that records which passengers survived the Titanic wreckage, it was able by itself to derive interesting trends like "passengers that paid higher fares were more likely to survive" or "survival rate was much higher for women than men".

The cookbook even lets the agent built its own submission to the challenge, and it ranks under 3,000 out of 17,000 submissions: ğŸ‘ not bad at all!

> - Try it for yourself in this Space demo ğŸ‘‰ https://lnkd.in/gzaqQ3rT
> - Read the cookbook to dive deeper ğŸ‘‰ https://lnkd.in/gXx3-AyH
{: .prompt-info}




<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

## ë°©ê¸ˆ Transformers Agentsë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ RAG(Retrieval Augmented Generation)ë¥¼ ì‰½ê²Œ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ìƒˆë¡œìš´ ì¿¡ë¶ì„ ì¶œíŒí–ˆìŠµë‹ˆë‹¤.

Vanilla RAGì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì œí•œ ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤.
- â¤ ì†ŒìŠ¤ ë¬¸ì„œë¥¼ í•œ ë²ˆë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì¶©ë¶„íˆ ê´€ë ¨ì„±ì´ ì—†ìœ¼ë©´ ìƒì„±ì´ ë‚˜ë¹ ì§ˆ ê²ƒì…ë‹ˆë‹¤.
- â¤ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì€ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì°¸ì¡°ë¡œ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë˜ë©°, ì´ëŠ” ì¢…ì¢… ì°¨ì„ ì±…ì…ë‹ˆë‹¤: ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ì ì¿¼ë¦¬ëŠ” ëŒ€ë¶€ë¶„ ì§ˆë¬¸ì´ê³  ì‹¤ì œ ë‹µë³€ì„ í¬í•¨í•˜ëŠ” ë¬¸ì„œëŠ” ê¸ì • ìŒì„±ì´ë¯€ë¡œ ìœ ì‚¬ì„± ì ìˆ˜ëŠ” ì˜ë¬¸ í˜•ì‹ì˜ ê´€ë ¨ì„±ì´ ë‚®ì€ ì†ŒìŠ¤ ë¬¸ì„œì— ë¹„í•´ ë‹¤ìš´ê·¸ë ˆì´ë“œë˜ì–´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì„ íƒí•˜ì§€ ì•Šì„ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.

RAG ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ë©´(ì•„ì£¼ ê°„ë‹¨í•˜ê²Œ, ë¦¬íŠ¸ë¦¬ë²„ ë„êµ¬ë¡œ ë¬´ì¥í•œ ì—ì´ì „íŠ¸) ì´ ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ëª¨ë‘ ì™„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
- âœ… ì¿¼ë¦¬ ìì²´ë¥¼ ê³µì‹í™”í•©ë‹ˆë‹¤(ì¿¼ë¦¬ ì¬êµ¬ì„±).
- âœ… í•„ìš”í•œ ê²½ìš° ë‹¤ì‹œ ê²€ìƒ‰í•  ì½˜í…ì¸  ë¹„íŒ(ìì²´ ì¿¼ë¦¬)Critique the content to re-retrieve if needed (self-query)

ì´ ì—ì´ì „íŠ¸ ì„¤ì •ì´ ê²°ê³¼ë¥¼ ì–¼ë§ˆë‚˜ ê°œì„ í•©ë‹ˆê¹Œ? ìš”ë¦¬ì±…ì— Llama-3-70Bë¥¼ ì‚¬ìš©í•˜ëŠ” LLM-as-a-judgeì˜ í‰ê°€ ë¶€ë¶„ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. ë°”ë‹ë¼ì—ì„œ ì—ì´ì „íŠ¸ RAGë¡œ ì „í™˜í•˜ë©´ ì ìˆ˜ê°€ 8.5% ì¦ê°€í•©ë‹ˆë‹¤! ğŸ’ª
(70.0%ì—ì„œ 78.5%ë¡œ)

í•˜ì§€ë§Œ í•œ ê°€ì§€ ì¤‘ìš”í•œ ë‹¨ì ì€, ì‹œìŠ¤í…œì´ 1ì´ ì•„ë‹Œ ì—¬ëŸ¬ LLM í˜¸ì¶œì„ í•˜ê¸° ë•Œë¬¸ì— RAG ì‹œìŠ¤í…œì˜ ëŸ°íƒ€ì„ë„ ì¦ê°€í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì ì ˆí•œ ì ˆì¶©ì•ˆì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤!

</details>