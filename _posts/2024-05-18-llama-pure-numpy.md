---
title: Llama 3 implemented in pure NumPy
description: Llama, Numpy
categories: [LLM, Llama]
tags: [Llama, Numpy]
# author: foDev_jeong
date: 2024-05-18 15:20:00 +0800
mermaid: true
# render_with_liquid: false
# image:
#   path: /assets/img/llm/LLM_evaluation_rank.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [Rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ]
---


## ğŸ¦™ Llama 3 Implemented in Pure NumPy: Understanding LLMs from First Principles

*Curiosity:* How can we understand LLM architectures at the most fundamental level? What insights can we retrieve by implementing complex models using only basic NumPy operations?

**Implementing Llama 3 in pure NumPy** offers a unique opportunity to understand transformer architectures from first principles. This educational implementation, inspired by Andrej Karpathy's work, provides clarity and interpretability that high-level frameworks often obscure.

### Why Pure NumPy Implementation?

```mermaid
graph LR
    A[Understanding LLMs] --> B[High-Level Frameworks]
    A --> C[Pure NumPy]
    
    B --> B1[Fast Development]
    B --> B2[Abstraction Layers]
    B --> B3[Less Visibility]
    
    C --> C1[Full Control]
    C --> C2[Clear Operations]
    C --> C3[Educational Value]
    
    style A fill:#e1f5ff
    style C fill:#fff3cd
    style B3 fill:#f8d7da
    style C3 fill:#d4edda
```

### Llama 3 Training Scale

*Retrieve:* The scale of Llama 3 training reveals the computational resources required for state-of-the-art models.

| Resource | Quantity | Impact |
|:---------|:---------|:-------|
| **GPUs** | 24,000 | Massive parallel processing |
| **Training Data** | 15T tokens | Comprehensive knowledge base |
| **Instruction Data** | 10M samples | Fine-tuning for alignment |
| **GPU Hours** | 1.3M hours | Extensive compute investment |

### Architecture Overview

**Key Insight**: Despite transitioning to GQA (Grouped Query Attention), the model structure remains unchanged from Llama 2, making it a familiar yet powerful framework.

```python
import numpy as np

class Llama3Attention:
    """Simplified Llama 3 attention mechanism in NumPy"""
    
    def __init__(self, dim, num_heads, head_dim):
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = head_dim
        
        # Weight matrices
        self.q_proj = np.random.randn(dim, num_heads * head_dim) * 0.02
        self.k_proj = np.random.randn(dim, num_heads * head_dim) * 0.02
        self.v_proj = np.random.randn(dim, num_heads * head_dim) * 0.02
        self.o_proj = np.random.randn(num_heads * head_dim, dim) * 0.02
    
    def forward(self, x, mask=None):
        """Forward pass of attention"""
        batch_size, seq_len, _ = x.shape
        
        # Project to Q, K, V
        Q = x @ self.q_proj  # [batch, seq, num_heads * head_dim]
        K = x @ self.k_proj
        V = x @ v_proj
        
        # Reshape for multi-head attention
        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        K = K.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        V = V.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # Scaled dot-product attention
        scores = np.einsum('bshd,bthd->bsht', Q, K) / np.sqrt(self.head_dim)
        
        if mask is not None:
            scores = np.where(mask, scores, -np.inf)
        
        attn_weights = self.softmax(scores)
        output = np.einsum('bsht,bthd->bshd', attn_weights, V)
        
        # Reshape and project output
        output = output.reshape(batch_size, seq_len, -1)
        return output @ self.o_proj
    
    def softmax(self, x):
        """Numerically stable softmax"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
```

### GQA (Grouped Query Attention) Implementation

**Note**: While GQA is incorporated into the code structure, it's not applied to model behavior in this educational implementation, ensuring enhanced interpretability.

```mermaid
graph TB
    A[Input Tokens] --> B[Embedding Layer]
    B --> C[Transformer Blocks]
    C --> D[Attention Layer]
    D --> E[GQA Mechanism]
    E --> F[Feed Forward]
    F --> G[Output]
    
    H[Positional Encoding] --> C
    I[Layer Norm] --> D
    I --> F
    
    style A fill:#e1f5ff
    style D fill:#fff3cd
    style E fill:#d4edda
    style G fill:#f8d7da
```

### Implementation Benefits

| Aspect | Benefit | Impact |
|:-------|:--------|:-------|
| **Educational** | Clear understanding of operations | â¬†ï¸ Learning |
| **Interpretability** | See every computation step | â¬†ï¸ Debugging |
| **Portability** | No framework dependencies | â¬†ï¸ Accessibility |
| **Clarity** | Intuitive model structure | â¬†ï¸ Understanding |

### Key Implementation Details

**1. Model Conversion**
- Leveraging stories15M model trained by Andrej Karpathy
- Converting to NumPy compressed format
- Maintaining clarity and precision

**2. Architecture Preservation**
- Llama 2 structure compatibility
- GQA integration (structural, not behavioral)
- Transformer block implementation

**3. Educational Focus**
- Step-by-step operations
- Clear mathematical formulations
- Practical examples

### Comparison: Framework vs. Pure NumPy

| Feature | PyTorch/TensorFlow | Pure NumPy |
|:--------|:-------------------|:-----------|
| **Speed** | âš¡âš¡âš¡ Very Fast | âš¡ Slower |
| **GPU Support** | âœ… Native | âŒ CPU only |
| **Abstraction** | High | Low |
| **Understanding** | âš ï¸ Limited | âœ… Complete |
| **Educational Value** | âš ï¸ Medium | âœ… High |

### Use Cases

*Retrieve:* Pure NumPy implementations are valuable for:
- Educational purposes
- Understanding model internals
- Debugging and verification
- Research and experimentation

*Innovate:* By understanding the fundamentals, you can:
- Optimize implementations
- Create custom architectures
- Debug complex issues
- Build domain-specific models

### Resources

**ğŸ§‘â€ğŸ’» Code Repository**: <https://github.com/likejazz/llama3.np>

**Key Features**:
- Pure NumPy implementation
- Educational focus
- Clear documentation
- Karpathy-inspired approach

### Key Takeaways

*Retrieve:* Implementing Llama 3 in pure NumPy provides deep insights into transformer architectures, revealing the mathematical operations that power modern LLMs.

*Innovate:* By understanding these fundamentals, you can innovate on architectures, optimize implementations, and build custom solutions tailored to specific needs.

*Curiosity â†’ Retrieve â†’ Innovation:* Start with curiosity about how LLMs work, retrieve knowledge through hands-on implementation, and innovate by applying these insights to new problems.

**Next Steps**:
- Explore the GitHub repository
- Run the implementation
- Modify and experiment
- Build your own variations 


<object data="/assets/img/llm/Llama3_implemeted_in_pure_Numpy.pdf" width="100%" height="450" type='application/pdf'></object>


<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

ğŸ¦™ ìˆœìˆ˜ NumPyğŸ‘© ğŸ”¬ë¡œ êµ¬í˜„ëœ ë¼ë§ˆ 3

ğŸš€ í¥ë¯¸ ì§„ì§„í•œ ë°œê²¬! @Andrej Karpathyì—ì„œ ì˜ê°ì„ ë°›ì•„ NumPyì—ì„œ êµ¬í˜„ ëœ Llama 3 ëª¨ë¸ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ ê¸°ì‚¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. AI at Meta ì˜ ë¼ë§ˆ 3 ëª¨ë¸ì€ ì¸ìƒì ì¸ ê·œëª¨ì™€ ì„±ëŠ¥ìœ¼ë¡œ íŒŒì¥ì„ ì¼ìœ¼í‚¤ê³  ìˆìŠµë‹ˆë‹¤. ğŸŒŸ

ğŸ§‘ ì½”ë“œ : <https://github.com/likejazz/llama3.np>

ğŸ” 24K GPU, 15T í›ˆë ¨ ë°ì´í„°, 10M ëª…ë ¹ ë°ì´í„° ë° 1.3M GPU ì‹œê°„ì„ ì‚¬ìš©í•˜ë©´ ê·¸ ìˆ˜ì¹˜ëŠ” ì •ë§ ì••ë„ì ì…ë‹ˆë‹¤. GQAë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ ì „í™˜í–ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ëª¨ë¸ êµ¬ì¡°ëŠ” Llama 2ì—ì„œ ë³€ê²½ë˜ì§€ ì•Šì•„ ì¹œìˆ™í•˜ë©´ì„œë„ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ§  ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ì €ìëŠ” NumPyë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê²Œ êµ¬í˜„í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤. Andrej Karpathyê°€ í›ˆë ¨í•œ stories15M ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë³´ë‹¤ ì§ê´€ì ì¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ ìœ„í•´ NumPy ì••ì¶• í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. Karpathyê°€ í›ˆë ¨í•œ Llama 2 ëª¨ë¸ì„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¡œ ë³€í™˜í•˜ì—¬ ì ‘ê·¼ ë°©ì‹ì˜ ëª…í™•ì„±ê³¼ ì •ë°€ë„ë¥¼ ìœ ì§€í•˜ëŠ” ë™ì•ˆ ê³„ì† ì§€ì¼œë´ ì£¼ì‹­ì‹œì˜¤.

ğŸ“Š GQAë¥¼ ì½”ë“œì— í†µí•©í•˜ëŠ” ë™ì•ˆ ì‘ì„±ìëŠ” GQAë¥¼ ëª¨ë¸ ë™ì‘ì— ì ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì´ê¸° ìœ„í•´ NumPyë¥¼ ì›í™œí•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•œ ë” ë§ì€ í†µì°°ë ¥ì„ ê³„ì† ì§€ì¼œë´ ì£¼ì‹­ì‹œì˜¤! 

</details>

