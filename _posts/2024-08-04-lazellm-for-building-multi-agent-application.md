---
title: 🗞 LazyLLM looks like a super simple low-code open-source framework for building multi-agent applications
description: LLM, LazyLLM
categories: [LLM, LazyLLM]
tags: [Multi-agent, LazyLLM]
# author: foDev_jeong
date: 2024-08-04 20:00:00 +0800
# pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---

![ A Low-code Development Tool For Building Multi-agent LLMs Application ](/assets/img/llm/lazyllm-for-multi-agent.jpeg){: .light .shadow .rounded-10 w='1212' h='668' }

## 💡LazyLLM supports a variety of applications that leverage multi-agent systems and LLMs like RAG, fine-tuning, content-generation etc. 

and defines workflows such as pipeline, parallel, diverter, if, switch, and loop etc.

### ⛳ These are the features that struck to me most: 

- 👉 LazyLLM allows developers to assemble AI applications easily using pre-built modules and data flows, even if they are not familiar with large models.
- 👉 The framework offers a one-click deployment feature which can be particularly useful during the Proof of Concept phase, as it manages the sequential start-up and configuration of various services like LLMs and embeddings.
- 👉 It supports deployment across different infrastructure platforms such as bare-metal servers, Slurm clusters, and public clouds.
- 👉 It has a built-in support for grid search and allows automatic exploration of different model configurations, retrieval strategies, and fine-tuning parameters to optimize application performance

The features aren't flashy or anything new, but LazyLLM is built in a straightforward way. 

The repository is pretty new and actively being developed, but it's nice to see so many low-code approaches being built. It helps developers from diverse backgrounds quickly build a prototype

> Github 👉 <https://github.com/LazyAGI/LazyLLM>
{: .prompt-info}