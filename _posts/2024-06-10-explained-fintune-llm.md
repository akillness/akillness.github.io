---
title: 5 techniques to fine-tune LLMs, explained visually!
description: LLM, Finetuning, Visualization
categories: [LLM, Finetuning]
tags: [LLM, Finetuning]
# author: foDev_jeong
date: 2024-06-10 10:10:00 +0800
# pin: true
# math: true
# mermaid: true
# image:
#   path: /assets/img/cover/programming.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [2024 programming curriculum by honglab]
---

## Fine-tuning large language models traditionally involved adjusting billions of parameters, demanding significant computational power and resources. 

However, the development of some innovative methods have transformed this process. 

Hereâ€™s a snapshot of five cutting-edge techniques for finetuning LLMs, each explained visually for easy understanding.

#### LoRA:

- Introduce two low-rank matrices, A and B, to work alongside the weight matrix W.
- Adjust these matrices instead of the behemoth W, making updates manageable.

#### LoRA-FA (Frozen-A):

- Takes LoRA a step further by freezing matrix A.
- Only matrix B is tweaked, reducing the activation memory needed.

#### VeRA:

- All about efficiency: matrices A and B are fixed and shared across all layers.
- Focuses on tiny, trainable scaling vectors in each layer, making it super memory-friendly.

#### Delta-LoRA:

- A twist on LoRA: adds the difference (delta) between products of matrices A and B across training steps to the main weight matrix W.
- Offers a dynamic yet controlled approach to parameter updates.

#### LoRA+:

- An optimized variant of LoRA where matrix B gets a higher learning rate.
This tweak leads to faster and more effective learning.

Credits to Avi Chawla for great visualisation! ğŸ‘

![ Visualization 5 fine-tune LLMs  ](/assets/img/llm/LLM_Finetune_visualization.gif){: .light .shadow .rounded-10 w='1212' h='668' }

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

## ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ì „í†µì ìœ¼ë¡œ ìˆ˜ì‹­ì–µ ê°œì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì¡°ì •í•´ì•¼ í–ˆê¸° ë•Œë¬¸ì— ìƒë‹¹í•œ ê³„ì‚° ëŠ¥ë ¥ê³¼ ë¦¬ì†ŒìŠ¤ê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤. 

ê·¸ëŸ¬ë‚˜ ëª‡ ê°€ì§€ í˜ì‹ ì ì¸ ë°©ë²•ì˜ ê°œë°œë¡œ ì´ í”„ë¡œì„¸ìŠ¤ê°€ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤. 

ë‹¤ìŒì€ LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•œ 5ê°€ì§€ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ê°„ëµí•˜ê²Œ ì„¤ëª…í•œ ê²ƒìœ¼ë¡œ, ê° ê¸°ë²•ì€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

#### ë¡œë¼:

- ê°€ì¤‘ì¹˜ í–‰ë ¬ Wì™€ í•¨ê»˜ ì‘ë™í•˜ë„ë¡ ë‘ ê°œì˜ ë‚®ì€ ìˆœìœ„ í–‰ë ¬ Aì™€ Bë¥¼ ë„ì…í•©ë‹ˆë‹¤.
- ê±°ëŒ€ W ëŒ€ì‹  ì´ í–‰ë ¬ì„ ì¡°ì •í•˜ì—¬ ì—…ë°ì´íŠ¸ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### LoRA-FA(ëƒ‰ë™-A):

- í–‰ë ¬ Aë¥¼ ë™ê²°í•˜ì—¬ LoRAë¥¼ í•œ ë‹¨ê³„ ë” ë°œì „ì‹œí‚µë‹ˆë‹¤.
- ë§¤íŠ¸ë¦­ìŠ¤ Bë§Œ ì¡°ì •ë˜ì–´ í•„ìš”í•œ í™œì„±í™” ë©”ëª¨ë¦¬ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.

#### ë² ë¼:

- íš¨ìœ¨ì„±ì— ê´€í•œ ëª¨ë“  ê²ƒ: í–‰ë ¬ Aì™€ BëŠ” ê³ ì •ë˜ì–´ ìˆê³  ëª¨ë“  ê³„ì¸µì—ì„œ ê³µìœ ë©ë‹ˆë‹¤.
- ê° ë ˆì´ì–´ì—ì„œ ì‘ê³  í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ë§ ë²¡í„°ì— ì¤‘ì ì„ ë‘ì–´ ë©”ëª¨ë¦¬ ì¹œí™”ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.

#### ë¸íƒ€-ë¡œë¼:

- LoRAì˜ íŠ¸ìœ„ìŠ¤íŠ¸: í›ˆë ¨ ë‹¨ê³„ì—ì„œ í–‰ë ¬ Aì™€ Bì˜ ê³± ê°„ì˜ ì°¨ì´(ë¸íƒ€)ë¥¼ ì£¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ Wì— ì¶”ê°€í•©ë‹ˆë‹¤.
- íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ì— ëŒ€í•œ ë™ì ì´ë©´ì„œë„ ì œì–´ëœ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.

#### ë¡œë¼+:

- í–‰ë ¬ Bê°€ ë” ë†’ì€ í•™ìŠµë¥ ì„ ì–»ëŠ” LoRAì˜ ìµœì í™”ëœ ë³€í˜•ì…ë‹ˆë‹¤.
ì´ ì¡°ì •ì€ ë” ë¹ ë¥´ê³  íš¨ê³¼ì ì¸ í•™ìŠµìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.

</details>