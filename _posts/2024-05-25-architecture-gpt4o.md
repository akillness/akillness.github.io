---
title: Is this the architecture of OpenAI GPT-4o?
description: LLM, Course
categories: [Script, GPT4o]
tags: [GPT4o, LLM, Architecture]
# author: foDev_jeong
date: 2024-05-25 11:05:00 +0800
mermaid: true
# render_with_liquid: false
# image:
#   path: /assets/img/llm/LLM_evaluation_rank.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [Rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ]
---


## Uni-MoE: Unified Multimodal LLM Architecture (GPT-4o-like)

*Curiosity:* How can we build a unified model that handles audio, speech, image, text, and video? What architecture enables efficient multimodal learning?

**Uni-MoE** proposes an MoE-based unified Multimodal Large Language Model (MLLM) that can handle audio, speech, image, text, and video. üëÇüëÑüëÄüí¨üé• This architecture may be similar to GPT-4o's approach.

### Uni-MoE Overview

*Retrieve:* Understanding the unified multimodal architecture.

**Uni-MoE** is a native multimodal Mixture of Experts (MoE) architecture with a three-phase training strategy:
1. Cross-modality alignment
2. Expert activation
3. Fine-tuning with Low-Rank Adaptation (LoRA)

### Architecture Highlights

```mermaid
graph TB
    A[Uni-MoE Architecture] --> B[Modality-Specific Encoders]
    A --> C[Connectors]
    A --> D[MoE Layers]
    
    B --> B1[Audio Encoder]
    B --> B2[Speech Encoder]
    B --> B3[Image Encoder]
    B --> B4[Text Encoder]
    B --> B5[Video Encoder]
    
    C --> C1[Cross-Modality Alignment]
    D --> D1[Sparse Activation]
    D --> D2[Expert Routing]
    
    C1 --> E[Unified Representation]
    D1 --> E
    D2 --> E
    
    style A fill:#e1f5ff
    style B fill:#fff3cd
    style E fill:#d4edda
```

### Key Features

| Feature | Description | Benefit |
|:--------|:------------|:--------|
| **Unified Multimodal** | Handles 5 modalities | ‚¨ÜÔ∏è Versatility |
| **MoE Architecture** | Sparse expert activation | ‚¨ÜÔ∏è Efficiency |
| **Modality-Specific Encoders** | Specialized processing | ‚¨ÜÔ∏è Quality |
| **Connectors** | Cross-modality alignment | ‚¨ÜÔ∏è Integration |
| **LoRA Fine-tuning** | Efficient adaptation | ‚¨áÔ∏è Training cost |

### Three-Phase Training Strategy

*Retrieve:* Systematic training approach.

**Phase 1: Cross-Modality Alignment**
- Train connectors for different modalities
- Align representations across modalities
- Establish unified space

**Phase 2: Expert Activation**
- Modality-specific expert training
- Cross-modality instruction data
- Expert specialization

**Phase 3: LoRA Fine-tuning**
- Fine-tuning with LoRA
- Mixed multimodal data
- Efficient adaptation

**Training Pipeline**:

```mermaid
graph LR
    A[Phase 1:<br/>Cross-Modality Alignment] --> B[Phase 2:<br/>Expert Activation]
    B --> C[Phase 3:<br/>LoRA Fine-tuning]
    C --> D[Uni-MoE Model]
    
    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#d4edda
    style D fill:#f8d7da
```

### Performance Results

*Innovate:* Uni-MoE's impressive achievements.

**Results**:
- ‚úÖ Matches or outperforms other MLLMs on 10 tested vision and audio tasks
- ‚úÖ Outperforms existing unified multimodal models on comprehensive benchmarks
- ‚úÖ Efficient training and inference through sparse MoE
- ‚úÖ Unified representation across modalities

### Architecture Comparison

| Aspect | Traditional MLLMs | Uni-MoE | Advantage |
|:-------|:------------------|:--------|:----------|
| **Modalities** | Limited | 5 modalities | ‚¨ÜÔ∏è More |
| **Architecture** | Dense | Sparse MoE | ‚¨ÜÔ∏è Efficiency |
| **Training** | Single-phase | Three-phase | ‚¨ÜÔ∏è Better |
| **Efficiency** | Standard | Optimized | ‚¨ÜÔ∏è Faster |

### Why This Matters

*Retrieve:* Uni-MoE demonstrates the potential architecture for GPT-4o-like unified multimodal models.

**Implications**:
- Unified models can handle multiple modalities
- MoE enables efficient scaling
- Three-phase training optimizes learning
- LoRA enables efficient fine-tuning

### Resources

> **Resources**:
> - **üìÑ Paper**: <https://huggingface.co/papers/2405.11273>
> - **üíª GitHub**: <https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/tree/master/Uni_MoE_v2>
{: .prompt-info}

### Key Takeaways

*Retrieve:* Uni-MoE proposes a unified multimodal LLM architecture using MoE that handles audio, speech, image, text, and video through a three-phase training strategy.

*Innovate:* By using modality-specific encoders, connectors, and sparse MoE architecture, Uni-MoE achieves efficient training and inference while matching or outperforming other MLLMs, potentially revealing insights into GPT-4o's architecture.

*Curiosity ‚Üí Retrieve ‚Üí Innovation:* Start with curiosity about unified multimodal architectures, retrieve insights from Uni-MoE's approach, and innovate by applying similar techniques to your multimodal applications.

>
- Paper: <https://huggingface.co/papers/2405.11273>
- Github: <https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/tree/master/Uni_MoE_v2>
{: .prompt-info }

![ GPT4o Architecture ](/assets/img/news/GPT4o-Architecture.jpeg){: .light .w-75 .shadow .rounded-10 w='1212' h='668' }

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

Uni-MoEÎäî Ïò§ÎîîÏò§, ÏùåÏÑ±, Ïù¥ÎØ∏ÏßÄ, ÌÖçÏä§Ìä∏ Î∞è ÎπÑÎîîÏò§Î•º Ï≤òÎ¶¨Ìï† Ïàò ÏûàÎäî MoE Í∏∞Î∞ò ÌÜµÌï© MLLM(Multimodal Large Language Model)ÏùÑ Ï†úÏïàÌï©ÎãàÎã§. üëÇüëÑüëÄüí¨üé•

Uni-MoEÎäî Í∏∞Î≥∏ Î©ÄÌã∞Î™®Îã¨ MoE(Mixture of Experts) ÏïÑÌÇ§ÌÖçÏ≤òÎ°ú, ÍµêÏ∞® Î™®Îã¨Î¶¨Ìã∞ Ï†ïÎ†¨, Ï†ÑÎ¨∏Í∞Ä ÌôúÏÑ±Ìôî Î∞è LoRA(Low-Rank Adaptation)Î•º ÌÜµÌïú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÏùÑ Ìè¨Ìï®ÌïòÎäî 3Îã®Í≥Ñ ÍµêÏú° Ï†ÑÎûµÏùÑ Í∞ñÏ∂îÍ≥† ÏûàÏäµÎãàÎã§. ü§î

TLÏûÖÎãàÎã§. Î∞ïÏÇ¨:
- üöÄ Uni-MoEÎäî ÌÜµÌï© Î©ÄÌã∞Î™®Îã¨ ÌëúÌòÑÏùÑ ÏúÑÌï¥ Ïª§ÎÑ•ÌÑ∞Í∞Ä ÏûàÎäî Î™®Îã¨Î¶¨Ìã∞Î≥Ñ ÏóîÏΩîÎçîÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.
- üí° Ìö®Ïú®Ï†ÅÏù∏ ÌïôÏäµ Î∞è Ï∂îÎ°†ÏùÑ ÏúÑÌï¥ Ìù¨ÏÜå MoE ÏïÑÌÇ§ÌÖçÏ≤ò ÌôúÏö©
- üßë üè´ 3Îã®Í≥Ñ ÍµêÏú°: 1) Îã§ÏñëÌïú ÏñëÏãùÏóê ÎåÄÌïú Ïª§ÎÑ•ÌÑ∞ ÌïôÏäµ 2) ÍµêÏ∞® ÏñëÏãù ÏßÄÏπ® Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìïú ÏñëÏãùÎ≥Ñ Ï†ÑÎ¨∏Í∞Ä ÍµêÏú°. 3) ÌòºÌï© Îã§Ï§ë Î™®Îìú Îç∞Ïù¥ÌÑ∞ÏóêÏÑú LoRAÎ°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ï.
- üìä Uni-MoEÎäî 10Í∞úÏùò ÌÖåÏä§Ìä∏Îêú ÎπÑÏ†Ñ Î∞è Ïò§ÎîîÏò§ ÏûëÏóÖÏóêÏÑú Îã§Î•∏ MLLMÍ≥º ÏùºÏπòÌïòÍ±∞ÎÇò Îçî ÎÇòÏùÄ ÏÑ±Îä•ÏùÑ Î∞úÌúòÌï©ÎãàÎã§.
- üèÜ Ìè¨Í¥ÑÏ†ÅÏù∏ Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑú Í∏∞Ï°¥ ÌÜµÌï© Î©ÄÌã∞Î™®Îã¨ Î™®Îç∏ÏùÑ Îä•Í∞ÄÌï©ÎãàÎã§.

>
- Paper: <https://huggingface.co/papers/2405.11273>
- Github: <https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/tree/master/Uni_MoE_v2>
{: .prompt-info }

</details>