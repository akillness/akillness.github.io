---
title: Is this the architecture of OpenAI GPT-4o?
description: LLM, Course
categories: [Script, GPT4o]
tags: [GPT4o, LLM, Architecture]
# author: foDev_jeong
date: 2024-05-25 23:05:00 +0800
# mermaid: true
# render_with_liquid: false
# image:
#   path: /assets/img/llm/LLM_evaluation_rank.jpeg
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: [Rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ]
---


## Is this the architecture of OpenAI GPT-4o?

Uni-MoE proposes an MoE-based unified Multimodal Large Language Model (MLLM) that can handle audio, speech, image, text, and video. ðŸ‘‚ðŸ‘„ðŸ‘€ðŸ’¬ðŸŽ¥

Uni-MoE is a native multimodal Mixture of Experts (MoE) architecture with a three-phase training strategy that includes cross-modality alignment, expert activation, and fine-tuning with Low-Rank Adaptation (LoRA). ðŸ¤”

TL;DR:
ðŸš€ Uni-MoE uses modality-specific encoders with connectors for a unified multimodal representation.
ðŸ’¡ Utilizes sparse MoE architecture for efficient training and inference
ðŸ§‘â€ðŸ« Three-phase training: 1) Train connectors for different modalities 2) Modality-specific expert training with cross-modality instruction data. 3) Fine-tuning with LoRA on mixed multimodal data.
ðŸ“Š Uni-MoE matches or outperforms other MLLMs on 10 tested vision and audio tasks
ðŸ† Outperforms existing unified multimodal models on comprehensive benchmarks

>
Paper: <https://huggingface.co/papers/2405.11273>
Github: <https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/tree/master/Uni_MoE_v2>
{: .prompt-info }

![ GPT4o Architecture ](/assets/img/news/GPT4o-Architecture.jpeg){: .light .w-75 .shadow .rounded-10 w='1212' h='668' }

<details markdown="1">
<summary style= "font-size:24px; line-height:24px; font-weight:bold; cursor:pointer;" > Translate to Korean </summary>

* * * 

## ì´ê²ƒì´ OpenAI GPT-4oì˜ ì•„í‚¤í…ì²˜ìž…ë‹ˆê¹Œ?

 Uni-MoEëŠ” ì˜¤ë””ì˜¤, ìŒì„±, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ë° ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìžˆëŠ” MoE ê¸°ë°˜ í†µí•© MLLM(Multimodal Large Language Model)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ðŸ‘‚ðŸ‘„ðŸ‘€ðŸ’¬ðŸŽ¥

Uni-MoEëŠ” ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ MoE(Mixture of Experts) ì•„í‚¤í…ì²˜ë¡œ, êµì°¨ ëª¨ë‹¬ë¦¬í‹° ì •ë ¬, ì „ë¬¸ê°€ í™œì„±í™” ë° LoRA(Low-Rank Adaptation)ë¥¼ í†µí•œ ë¯¸ì„¸ ì¡°ì •ì„ í¬í•¨í•˜ëŠ” 3ë‹¨ê³„ êµìœ¡ ì „ëžµì„ ê°–ì¶”ê³  ìžˆìŠµë‹ˆë‹¤. ðŸ¤”

TLìž…ë‹ˆë‹¤. ë°•ì‚¬:
ðŸš€ Uni-MoEëŠ” í†µí•© ë©€í‹°ëª¨ë‹¬ í‘œí˜„ì„ ìœ„í•´ ì»¤ë„¥í„°ê°€ ìžˆëŠ” ëª¨ë‹¬ë¦¬í‹°ë³„ ì—”ì½”ë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ðŸ’¡ íš¨ìœ¨ì ì¸ í•™ìŠµ ë° ì¶”ë¡ ì„ ìœ„í•´ í¬ì†Œ MoE ì•„í‚¤í…ì²˜ í™œìš©
ðŸ§‘ ðŸ« 3ë‹¨ê³„ êµìœ¡: 1) ë‹¤ì–‘í•œ ì–‘ì‹ì— ëŒ€í•œ ì»¤ë„¥í„° í•™ìŠµ 2) êµì°¨ ì–‘ì‹ ì§€ì¹¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì–‘ì‹ë³„ ì „ë¬¸ê°€ êµìœ¡. 3) í˜¼í•© ë‹¤ì¤‘ ëª¨ë“œ ë°ì´í„°ì—ì„œ LoRAë¡œ ë¯¸ì„¸ ì¡°ì •.
ðŸ“Š Uni-MoEëŠ” 10ê°œì˜ í…ŒìŠ¤íŠ¸ëœ ë¹„ì „ ë° ì˜¤ë””ì˜¤ ìž‘ì—…ì—ì„œ ë‹¤ë¥¸ MLLMê³¼ ì¼ì¹˜í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.
ðŸ† í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ í†µí•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

>
Paper: <https://huggingface.co/papers/2405.11273>
Github: <https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/tree/master/Uni_MoE_v2>
{: .prompt-info }

</details>