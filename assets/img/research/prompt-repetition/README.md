# Prompt Repetition Research Figures

This directory contains figures from the research paper "Prompt Repetition Improves Non-Reasoning LLMs" (Leviathan et al., 2025).

## Required Images

Please add the following figures from the paper:

1. **figure1-accuracy-comparison.png** - Figure 1: Prompt repetition vs. baseline accuracy for popular LLMs and various benchmarks when asking models not to reason. Shows 47 wins out of 70 tests with 0 losses.

2. **figure2-comparison-part1.png** - Figure 2: Comparison of accuracy, average and median response length, as well as average latency across methods and benchmarks (Part 1).

3. **figure3-comparison-part2.png** - Figure 3: Comparison of accuracy, average and median response length, as well as average latency across methods and benchmarks (Part 2).

4. **figure4-reasoning-mode.png** - Figure 4: Prompt repetition vs. baseline accuracy when asking models to think step by step. Shows 5 wins, 1 loss, and 22 neutral results out of 28 tests.

## Source

Paper: [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982)
Authors: Yaniv Leviathan, Matan Kalman, Yossi Matias (Google Research)
Published: December 2025
